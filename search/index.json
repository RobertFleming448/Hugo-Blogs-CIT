[{"content":"Preface I am going to try something slightly different this week. As I don\u0026rsquo;t feel that my progress in any one project is substantial nor unique enough to fill the contents of a full blog, I am going to make this blog a double feature The first part will detail my exploits in Terraform to quickly build cloud architecture, and my second part will explain how I setup pf in freeBSD.\nTerraform Introduction This part will go over how I created the AWS infrastructure for our group project using Terraform. I was tasked with writing the compute section of our Terraform which includes:\n The Bastion Instances The Server Instances The Security Groups The Load Balancer  Setup Since I am writing this as a smaller part of a larger infrastructure, I needed to prepare a sample architecture on my personal AWS. I chose this method because I did not want to deal with group conflicts before integration testing. To accomplish this, I replicated the subnet CIDR and other important VPC characteristics manually. In the final project this will also be spun up with Terraform, but for my section that would not be required.\nSecurity Groups Security Groups in Terraform are rather self explanatory. Like everything in Ansible, they have optional parameters that can be specified to get exactly what you need. As mentioned in prior blogs, I needed to setup several security groups:\n Allow SSH from Team Allow All Web Traffic In Web Server Instance SG Bastion Instance SG  The Web Server SG is unique as it does not operate on IP CIDR ranges but on security groups. For this implementation, I want only Instances belonging to the Bastion SG SSH access to the Web Server Instance. A standard Security Group declaration looks like this:\nresource \u0026#34;aws_security_group\u0026#34; \u0026#34;T_allow_all_web\u0026#34; { name = \u0026#34;T_allow_all_web\u0026#34; description = \u0026#34;Allow http and https\u0026#34; vpc_id = \u0026#34;vpc-*****************\u0026#34; #replace with \u0026lt;vpc\u0026gt;.id in future ingress { description = \u0026#34;HTTP\u0026#34; from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } ingress { description = \u0026#34;HTTPS\u0026#34; from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;T_allow_all_web\u0026#34; } } Note the cidr_blocks section in the ingress rules. For my Web Server Group it is replaced by security_groups:\nresource \u0026#34;aws_security_group\u0026#34; \u0026#34;T_instance_group\u0026#34; { name = \u0026#34;T_instance_group\u0026#34; description = \u0026#34;Allow SSH from home IP\u0026#34; vpc_id = \u0026#34;vpc-*****************\u0026#34; #replace with \u0026lt;vpc\u0026gt;.id in future ingress { description = \u0026#34;SSH from bastion group\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; security_groups = [aws_security_group.T_bastion_group.id] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;T_instance_group\u0026#34; } } Since this group is already declared elsewhere in our Terraform we can refer to its ID using [aws_security_group.T_bastion_group.id]\nEC2 Terraform EC2 is something I have encountered in the past, but I never used a custom VPC in the past. Like the security groups, they optional vpc_id parameter is specified, however some other parameters are required to get desired function. First let\u0026rsquo;s take a look at one of the bastion\u0026rsquo;s terraform:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;T_Bastion_1\u0026#34; { ami = \u0026#34;ami-0ca5c3bd5a268e7db\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; key_name = \u0026#34;CIT480\u0026#34; associate_public_ip_address = \u0026#34;true\u0026#34; availability_zone = \u0026#34;us-west-2a\u0026#34; subnet_id = \u0026#34;subnet-*****************\u0026#34; #replace with \u0026lt;subnet\u0026gt;.id in future vpc_security_group_ids = [aws_security_group.T_allow_ssh.id, aws_security_group.T_bastion_group.id ] tags = { Name = \u0026#34;T_Bastion_1\u0026#34; } } For the AMI we are using Ubuntu 20.04. Its simply the Linux distribution the team is most comfortable with. It is worth mentioning the use of [vpc_security_group_ids] as opposed to [security_groups]. Since we are using a custom VPC this must be specified. I also prespecified a key already attached to my AWS account. To ensure this machine is directly reachable from outside the network I had it associate with a public IP address. I also attached the Security groups mentioned in the previous step.\nNext is the Server group: resource \u0026#34;aws_instance\u0026#34; \u0026#34;T_Server_1\u0026#34; { ami = \u0026#34;ami-0ca5c3bd5a268e7db\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; key_name = \u0026#34;CIT480\u0026#34; associate_public_ip_address = \u0026#34;false\u0026#34; availability_zone = \u0026#34;us-west-2a\u0026#34; subnet_id = \u0026#34;subnet-*****************\u0026#34; #replace with \u0026lt;subnet\u0026gt;.id in future vpc_security_group_ids = [aws_security_group.T_allow_all_web.id, aws_security_group.T_instance_group.id ] tags = { Name = \u0026#34;T_Server_1\u0026#34; } }\nThe notable difference in this is the different subnet id (which you cannot see), and not associating a public IP making this machine non-internet facing.\nLoad Balancer Of all the steps, this was the one that I was least familiar with, however it actually was much easier than anticipated. While browsing Terraform documentation, I found [aws_elb] resource. By default it constructed a classic load balancer. This is exactly what I wanted. All it required was the subnets, instances, listeners, health check settings, and security groups.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;T_Server_1\u0026#34; { resource \u0026#34;aws_elb\u0026#34; \u0026#34;T-Load-Balancer\u0026#34; { name = \u0026#34;T-Load-Balancer\u0026#34; subnets = [\u0026#34;subnet-*****************\u0026#34;, \u0026#34;subnet-*****************\u0026#34;, \u0026#34;subnet-*****************\u0026#34;] listener { instance_port = 80 instance_protocol = \u0026#34;http\u0026#34; lb_port = 80 lb_protocol = \u0026#34;http\u0026#34; } /* Add cert on main environment listener { instance_port = 80 instance_protocol = \u0026#34;http\u0026#34; lb_port = 443 lb_protocol = \u0026#34;https\u0026#34; ssl_certificate_id = \u0026#34;arn:aws:iam::123456789012:server-certificate/certName\u0026#34; } */ health_check { healthy_threshold = 10 unhealthy_threshold = 2 timeout = 5 target = \u0026#34;HTTP:80/index.html\u0026#34; interval = 30 } instances = [aws_instance.T_Server_1.id, aws_instance.T_Server_2.id, aws_instance.T_Server_3.id] security_groups = [aws_security_group.T_allow_all_web.id] tags = { Name = \u0026#34;T-Load-Balancer\u0026#34; } } For the time being I have the https listener commented out. I will add the required HTTPS certificate when testing on the main environment is feasible. I do not foresee this being much of an issue. My assumption is that it will function similar to referencing other existing AWS resources such as the RSA key.\nConclusion This process took a little bit of tinkering. Although I have done Terraform in the past, I was admittingly rusty. I am super thankful for the wonderful documentation Terraform provides. The structure is akin to Microsoft\u0026rsquo;s PowerShell documentation and I find it just as easy to read. After I ran the final Terraform file, and used Ansible to install the web servers, the load balancer showed all 3 instances as healthy. However for some reason the domain name of the Load Balancer would not display the page in my browser. I certainly hope this is a problem with my custom VPC and not my Compute terraform, and I also hope the member of my group writing the VPC terraform does not run into a similar issue. The process was not bad overall, and I am looking forward to testing it in a real environment.\nPacket Filter Introduction This section will be much less in depth than the prior. The lab involved installing and configuring pf (packet filter) on free-BSD, a modern Unix-like OS.\nKernel Config To use pf first it must be enabled in the Kernel.\n# All other Kernel Devices device pf device pflog Pflog is not mandatory, but will be required for a future lab so I chose to enable it now. Additionally, I enabled all the Ethernet devices that I had disabled earlier so that I could test if my new rules worked.\nAfter a rebuild (several actually) and about 30 minutes of waiting, pf was now available.\nEnabling PF Next to enable pf you must edit /etc/rc.conf This file is for changing system configurations. I added the following lines:\nsysrc pf_enable=yes sysrc pflog_enable=yes pflog_logfile=\u0026#34;var/log/pflog\u0026#34; pf_enable=\u0026#34;yes\u0026#34; pflog_enable=\u0026#34;yes\u0026#34; This would enable pf on reboot.\nAdding Rules The last step was to add rules. First I had to make the packet filter configuration file aka /etc/pf.conf. All I had to do next was follow the pf syntax and specify my rules:\nblock in all\t#Block all in by default pass out all keep state\t#Allow all out by default pass in proto tcp to port 22\t#SSH Port In pass out proto { tcp udp } to port 53 keep state\t#DNS Port Out The lab required us to allow SSH in and DNS out. If this were my personal configuration I would make the rules more stringent. I also chose to make the default block all as it is common practice. After a system reboot I tested my rules by pinging Google. Since my ping was a success, I was able to confirm DNS worked as intended. I was unable to test SSH but I feel that will be an issue for a future lab.\nConclusion So ends my double feature. This blog feels long, but really I think the code snippets are the main padding factor. I hope this blog was focused but still provided some variety. The difficulty is figuring out what I will write about next week. Never before have I actually wanted to run into a problem in CIT, but a blog opportunity certainly is promising.\n","date":"2021-04-08","permalink":"https://robertfleming448.github.io/post/481blog6/","tags":["CIT-481","AWS","Terraform","freeBSD","Packet Filter"],"title":"CIT 481 blog 6"},{"content":"Introduction As mentioned in the blog prior where I discussed Load Balancers and Certificates on AWS, this week I will be going over my solution for getting our SSH bastion to function as intended.\nSSH Bastion As a preface, an SSH Bastion is a remote server that resides in a public section of a network. By connecting to this node, you can further connect to other machines located in private, non-internet facing subnets.\nServer Setup An ssh bastion really is very similar to any other machine out there. All that really matters for our implementation is having a network with a private and public subnet. I will not go into the section as it was not my lead role in the project. The bastion resides in the public subnet so that us administrators can SSH into it, and the target server, must reside in the private subnet to increase its security.\nSecurity Groups This section applies to AWS in particular, but rules like these can be setup on really any firewall. First we must block all incoming connections by default. All traffic can be allowed out. After on our bastion group we must allow SSH traffic only through our home, or otherwise trusted IP address. Next we must configure our server group to only allow ssh traffic from the bastion group (or bastion IP\u0026rsquo;s if not done in AWS). Next you must allow web traffic out for the load balancer.\nNAT Your Private subnet needs to be able to connect to outside traffic so you must either route it to a NAT Gateway in your public subnet or a NAT Instance in your public subnet. They both function similarly, however a NAT instance requires more configuration. This NAT instance can also function as your SSH bastion. For the purpose of this example, I will be using a NAT Gateway like described in last weeks blog.\nEC2 Now you spin up the EC2 instances with the bastion in public with its security group, and the server in private with its security group. In theory this is all that is really required on the server infrastructure setup for this part. For ansible you will want to ensure that python is installed on your host machines.\nConnection Setup Now that we have a proper SSH setup, we need to be able to connect to both the bastion and be able to jump to the web server. We should have 2 IP addresses:\n The public IP of the bastion The private IP of the server  Since both machines reside in the same network, the bastion should be able to recognize the server exclusively by its private address. Now to actually connect to the ssh bastion is quite simple. So long as you have the correct key file you can ssh directly to it with the ssh -i \u0026lt;key path\u0026gt; \u0026lt;username\u0026gt;@\u0026lt;bastion host ip command. This should successfully connect you to the bastion. However if you try to jump to the private instance you\u0026rsquo;ll realize something: you don\u0026rsquo;t have the key! There are 2 ways of doing this for SSH\u0026rsquo;ing I\u0026rsquo;ll go over both methods:\n Using Secure Copy scp copy your SSH RSA key to the bastion then jump manually. Setup your OpenSSH config file to use built in proxy features (PICK ME!)  The syntax for OpenSSH configs is very simple, and even better yet, Ansible uses it by default.\n~/.ssh/config Setup First navigate to the ~/.ssh/ directory. If you do not have a file name config. make it. chmod 600 it to only allow owner read write access. Next make an entry the entry syntax for standard connections is:\nHost bastion1 HostName \u0026lt;bastion public ip\u0026gt; User \u0026lt;default username\u0026gt; IdentityFile \u0026lt;key file path\u0026gt; Next you must add the web server host that must jump from that connection:\nHost server1 HostName \u0026lt;server private ip\u0026gt; User \u0026lt;default username\u0026gt; IdentityFile \u0026lt;key file path\u0026gt; ProxyCommand ssh -q bastion1 nc %h %p Note the added line. This command instructs OpenSSH to first connect to the bastion1 host we defined earlier then jump to the server1 host using the specified key file. Now when we SSH we need not specify the key file or username. We can enter ssh bastion1 or ssh server1 and OpenSSH will handle the rest. This method allows us to keep the key on only our machine and not on the bastion host. Like mentioned previously, Ansible will work with this configuration as well. Now we can specify the host names we wrote in the config to the ansible inventory file and OpenSSH should handle the rest.\nConfig Automation Since the theme of the class and project in general is automation. I thought it would be wise to also automate the config generation process. I wrote a simple bash script to take IP addresses as parameters and generate a config entry. this entry can be created in the current directory, appended to the config file or replace it entirely through prompts. I am in the process of adding options to the script to allow it to be ran automatically without user interaction. More on that in a future blog. Here is the code for the script: #! /bin/bash  keyName=\u0026#34;Robert_Test.pem\u0026#34; position=0 counter=0 if [ $(($#%2)) -ne 0 ] then echo \u0026#34;Even amount of args required\u0026#34; exit fi echo -e \u0026#34;SSH config file will be generated with: \\n\u0026#34; itemArray=() for item in \u0026#34;$@\u0026#34; do itemArray+=( $item ) done while [ $counter -lt $# ] do let position=position+1 echo -e \u0026#34;SSH Bastion $position:\\t ${itemArray[$counter]}\u0026#34; let counter=counter+1 echo -e \u0026#34;Web Server $position:\\t ${itemArray[$counter]}\u0026#34; let counter=counter+1 done echo -e -n \u0026#34;\\nIs this okay (y/n) \u0026gt;\u0026#34; read answer if [[ $answer != \u0026#34;y\u0026#34; ]] then echo \u0026#34;generation cancelled\u0026#34; exit fi answer=\u0026#34;\u0026#34; contents=\u0026#34;###config start\\n\u0026#34; position=0 counter=0 echo -e \u0026#34;generated contents are:\\n====================================================\\n\u0026#34; while [ $counter -lt $# ] do let position=position+1 contents=\u0026#34;$contents\\n\\n### Bastion $position, Directly Reachable\\nHost bastion$position\\nHostName${itemArray[$counter]}\\nUser ubuntu\\nIdentityFile ~/.ssh/$keyName\\n\u0026#34; let counter=counter+1 contents=\u0026#34;$contents\\n### Web Server $position, Reachable after 1 jump\\nHost server$position\\nHostName ${itemArray[$counter]}\\nUser ubuntu\\nIdentityFile ~/.ssh/$keyName\\nProxyCommand ssh -q bastion$positionnc %h %p\u0026#34; let counter=counter+1 done echo -e $contents echo -e \u0026#34;\\n====================================================\\n\u0026#34; echo -e -n \u0026#34;Create file here (h), append to ssh config (a), overwrite ssh config (o) or cancel (c)?\\n\u0026gt;\u0026#34; read answer if [[ $answer == \u0026#34;h\u0026#34; ]] then echo -e \u0026#34;$contents\u0026#34; \u0026gt; config chmod 600 config echo \u0026#34;Config generated in current directory\u0026#34; elif [[ $answer == \u0026#34;a\u0026#34; ]] then echo -e \u0026#34;\\n$contents\u0026#34; \u0026gt;\u0026gt; ~/.ssh/config echo \u0026#34;Config appended to ~/.ssh/config\u0026#34; elif [[ $answer == \u0026#34;o\u0026#34; ]] then answer=\u0026#34;\u0026#34; echo -e -n \u0026#34;Are you sure you want to overwrite the contents of ~/.ssh/config? this cannot be undone (y/n)\\n\u0026gt;\u0026#34; read answer if [[ $answer != \u0026#34;y\u0026#34; ]] then echo \u0026#34;generation cancelled\u0026#34; exit fi echo -e \u0026#34;$contents\u0026#34; \u0026gt; ~/.ssh/config chmod 600 ~/.ssh/config echo \u0026#34;Config overwrote ~/.ssh/config\u0026#34; else echo \u0026#34;generation cancelled\u0026#34; exit fi\nDo keep in mind that I don\u0026rsquo;t claim the be the best \u0026ldquo;bash scripter\u0026rdquo; in the world. So there is likely some better way of reading arguments that I\u0026rsquo;m not aware of. However it works and we can now quickly generate configs for quick SSH access.\nConclusion In the end, this solution was by far the easiest. I was searching for a method online and I found various other answers, but they all involved editing Ansible configuration extensively. I thought this method was more in line with the \u0026ldquo;straight out of the box\u0026rdquo; approach that is lent to quickly building infrastructure, so I\u0026rsquo;m glad it worked. While this solution may seem simple it did take several hours of research to discover. The main breakthrough came about when I learned Ansible utilized OpenSSH so default configs should work. From there I just had to discover the proxy syntax and it worked like a charm.\n","date":"2021-03-31","permalink":"https://robertfleming448.github.io/post/481blog5/","tags":["CIT-481","AWS","Ansible","SSH"],"title":"CIT 481 blog 5"},{"content":"Introduction In this blog I will record my issues and solutions I went through while trying to complete the first module of our CIT 481 Senior Design project.\nBackground Our team is undergoing an alternative project offered by the professor and we are tasked with using an Ansible playbook to automate building a tech stack on an AWS infrastructure. This infrastructure includes 3 private and 3 public subnets spanning across 3 separate availability zones. Each Availability zone must contain a private web server EC2 instance and a public SSH bastion Host EC2 instance. I will focus on the computing resources in this blog and less so on the VPC as that was a lesser role of mine.\nAWS Infrastructure Our infrastructure went through a few different version. At first our model was very poor. We had little idea of where components would go in our model and we were unaware of the routing processes. The first model we developed is bellow.\nAs you can see, we were unsure about how an SSH bastion even functioned in practice. We had it placed outside the VPC in a sort of purgatory due to this. After trying this model in AWS, it was obvious that it would not work. We went back to the drawing board and reviewed class content and we discovered how wrong we were. The updated model is bellow.\nThis time we were much more defined, and we understood where the SSH bastion(S) should be placed. Additionally we determined that our previous model involving an elastic IP address connected to a load balancer was not how AWS functioned. Lastly we added NAT gateways to the Public subnet to provide outside access to Private resources.\nNAT Gateway When the SSH Bastion was configured and worked as intended (More on that in a future blog). I successfully connected to and attempted an Apt-Update. To my dismay, it could not connect to the internet. This was remedied by routing it to a NAT gateway that I placed in the Public Subnet of that availability zone. Once I specified all traffic to be routed through each corresponding NAT gateway, both Apt-Updates, and other Ansible calls could perform successfully.\nUsing Our Domain In a previous blog I described how I used an A record to point to an Elastic IP address pointing to an EC2 instance to test our TLS certificates and Domain name. However once the load balancer was issued, we could no longer use this method. To use our purchased GoDaddy domain in AWS we needed to setup the AWS name server records in our Godaddy domain management console. This gave AWS Route 53 control of making future records. Next I made an alias for our domain to allow traffic from www.teambellevue.xyz to reach the same host. I then added another record to attach my load balancer to the hosted zone. Once all the records were in place, the domain pointed to our load balancer with all three servers providing.\nIssues Encountered When we were setting up a classic load balancer we could not for the life of us get it to register as healthy. Initially I had thought this was due the private nature of subnet causing issues with a classic load balancer. I tried the other load balancer options yet still no luck. After a consultation with the Professor, we discovered a simple mistake; The EC2 instances Security settings blocked incoming HTTP/HTTPS. I had thought that since all traffic was routed to the NAT gateway this would suffice, but that was not the case. After assigning it another security group, I remade the load balancer and pointed it towards the 3 web servers, and it displayed them all as healthy.\nTLS Certificate with a Classic Load Balancer Another issue that sprang up that I had not anticipated was the TLS certificate acquisition. I had achieved this earlier on a single instance using Ansible and Certbot to acquire a free TLS Certificate from Let\u0026rsquo;s Encrypt, however the method was no longer working with an Elastic Load Balancer. Although I am not entirely sure, my suspicion is that the method Certbot uses for authentication would not work with a classic load balancer. In my previous method, certbot was able to identify the IP address with the A record in my GoDaddy console, however this time since I was using an AWS hosted zone there was not IP address to input as an A record. My guess, I could be wrong, is that AWS just forwards traffic to the load balancer domain and does not have an actual public IP address for it. This would make sense since I was never able to assign it an elastic IP.\nTo fix this problem I looked into Amazon Certificate Management Service. Luckily for us AWS will provide TLS certificates to AWS compute services free of charge. We will only need to pay for the compute resources themselves. After inputting my domain info and other equivalent aliases, a cert was generated for me. Next I went back to my load balancer and enabled HTTPS listening. This prompted the creation wizard to have me select my certificate, which I chose the AWS managed one. It is worth mentioning that the Let\u0026rsquo;s Encrypt cert would work here if I chose to import it, but I feel that makes the process overly complicated. After it finished setting up, all 3 servers listed healthy and I tested my domain for https access. At first it did not work but after a few moments and refreshed later the lock icon appeared in the searchbar.\nConclusion This step through me a couple of curve balls. Never have I worked with private subnet resources in AWS before. This made finding out how route tables and NAT gateways worked very rewarding. I was also pleased how simple AWS made Cert acquisition. Making that process automatic in Ansible took much more time and was not all that reliable. I feel this method is much better than the last, and I am glad I forced to discover it. While Project 0 is now concluded, I feel that my role as leader will still be active to a degree. Next blog I will delve into the lengthy process I went through try and get SSH bastion hopping automated with Ansible.\n","date":"2021-03-25","permalink":"https://robertfleming448.github.io/post/481blog4/","tags":["CIT-481","AWS","TLS","Ansible","SSH"],"title":"CIT 481 blog 4"},{"content":"Introduction For this blog I will be going over my process and reasoning for creating a Minecraft Server with a Raspberry Pi 4. This is one of the topics I have saved for moments that are somewhat slow this semester. While I have been working diligently on other projects, the content is not substantial enough to fill an entire blog. As a result, I will keep it on the backburner until I feel suitable progress has been made.\nReasoning During this pandemic, I find interacting with friends online to be a more pressing matter. Without avenues to interact with my peers, I would surely go mad. Like many friend groups, we enjoyed playing Minecraft casually together. To do this one of our friends purchased a \u0026ldquo;Minecraft Realm\u0026rdquo; subscription. This service grants players access to a multiplayer server for a monthly fee. However when our friend no longer was interested in playing he discontinued the service. This left everyone else in the friend group stranded. We either had to pay for the server, or host it ourselves. I decided that since I was the most technical person of the group, I would be the one to host the server, and save us money in the process.\nThe Hardware To run a Minecraft server, one really only needs a device with Java installed, and the server side software. I knew that I didn\u0026rsquo;t want to run the software on my primary computer as it would be overkill, and I don\u0026rsquo;t want my 850 watt pc running nonstop 24/7. After some research, I decided a Raspberry Pi 4 with 8gb of RAM should be sufficient for the few users that still played.\nThe OS Raspberry Pi\u0026rsquo;s are very flexible so I decided to use an OS I was already comfortable with. I chose Ubuntu\u0026rsquo;s Raspberry Pi distribution desktop environment. I understand that running in a command line only server OS would save system resources, but frankly for system setup, I much prefer a GUI. I disabled the GUI on startup after the configuration changes were made.\nConfiguring the Server Configuring the server was rather simple for the most part. First I installed GUFW. For those not in the loop, GUFW or Graphic Uncomplicated Firewall, is an application that runs on top of the standard UFW or Uncomplicated Firewall. Despite UFW being much easier than IP tables, I still find setting rules much easier in the GUI environment. Next I installed Samba through Ubuntu\u0026rsquo;s built-in file sharing interface. I wanted to set this up so that I could transfer files to and from the server within my LAN. Lastly I installed Java along with the Minecraft server Jar. Using GUFW I allowed SMB and SSH traffic through my LAN, for server management. I also allowed the Minecraft server default port (25565) through to ensure clients could connect to the server.\nNetwork configuration I am certain I have gone over this before so I will make it brief. I went to my Router\u0026rsquo;s default gateway page using ipconfig via Windows CMD to locate its address. Once I navigated to it in a web browser I inputted the Administrator Credentials and added a new virtual server for port forwarding. In the entry I chose the Private IP I wanted for the server as well as which ports I wanted to allow traffic to. Lastly I went to my reserved IP address section in my Router and reserved the previous IP for the Raspberry Pi\u0026rsquo;s MAC address. This would ensure that DHCP would not give my Server a different Internal IP address when reconnecting to the network. Lastly, I went to CanYouSeeMe.org to see if traffic was allowed through the port. The tool confirmed traffic was allowed through Success.\nControlling the server Now that I had the server all set up, I needed to be able to access it remotely. First I wanted to test the shared network folder that I had enabled with SMB. I went to my Windows explorer and selected the Map Network Drive option. After typing in my Raspberry Pi credentials, I could see the shared folder. Next I needed a method of SSH\u0026rsquo;ing into the server. I had considered using an SSH application like MobaXterm or PuTTY, but after a little searching I found that PowerShell had an optional SSH module. After checking my Windows optional features, I was able to confirm the option was enabled. Next I typed in ssh \u0026lt;username\u0026gt;@\u0026lt;ip address\u0026gt; I was immediately prompted for my credentials, and when I entered them I was given full access. I didn\u0026rsquo;t mention this prior, but I made a separate account, I was not running Root the entire time. This user account was a sudoer for whenever I needed elevated privileges however.\nFinishing Touches Now that I confirmed everything was working, I needed the files for our original server. Luckily our friend was allowed to download them before he cancelled his subscription. With a little bit of rearranging, I was able to get all of our old server\u0026rsquo;s files in their proper place. It is worth noting that Minecraft Realms has a slightly different organization method than the Server API we were using. Next I installed a few useful Server Plugins via my SMB shared folder. After that I turned the server on with a script over SSH. Once it was running, all our friends joined and it was working as well as could be expected. A Raspberry Pi is a tiny computer after all. However for our standard concurrent player count of around 3 it was plenty sufficient.\nConclusion This project was a great success. The server may be slightly underpowered, but for a little fun here and there it works fine. I did a little calculations and the Raspberry Pi only costs about 5 dollars a year to run in power. That with the price of the computer itself costs around only 80 dollars. That may seem a bit much, but compared to the subscription fee, it will pay for itself in no time. And if I ever want to repurpose the Pi for a future project I always have that option.\n","date":"2021-03-10","permalink":"https://robertfleming448.github.io/post/481blog3/","tags":["CIT-481","SMB","Server","UFW","SSH"],"title":"CIT 481 blog 3"},{"content":"Introduction This blog I will be detailing the process I went through to acquire a domain and a TLS certificate to allow HTTPS traffic. This will be the first part in a potential multipart series going through my tasks in the CIT 481 semester-long group project. I was put in charge of Domain and Certificate Acquisition as I was the one with the most experience with domains.\nDomain Choice Being poor college students, we didn\u0026rsquo;t want to break the bank on our domain. However, we still wanted it to be related to our team, to make it easily distinguishable. With those prerequisites in mind, I went searching several domain registrar websites to find a cheap year long domain license. The main options I looked through were:\n Amazon Route 53 Godaddy NameCheap  I decided to go with Godaddy because I was already experienced with them, and they still offered prices competitive with the other registrars. In the end I chose teambellevue.xyz for the low low price of .99 for the first year.\nDomain Configuration Next I needed to put the domain in use. For the time being I knew I wanted to test our HTTPS connection, so I chose to spin up a temporary infrastructure just to learn the process.\nAWS Setup First I created a new security group that would allow SSH traffic in for shell access, and allow HTTP 80 and HTTPS 443. I had a similar security group before that only allowed HTTP but this would require more rules to allow secure traffic as well.\nNext I spun up a t2.micro instance using the Ubuntu 20.04 AMI. I added the security group and specified which key file I wanted to get SSH access.\nLastly, I allocated a new elastic IP address and assigned my running EC2 instance to it. I noted the IP address for use in my godaddy domain management console.\nGodaddy Setup This step was very simple to do manually, but in the future automating it may be a little more difficult. I simply went to my godaddy console and added a new a record to connect the domain to the IP address.\nEC2 Configuration For this step I installed Apache2 and Nginx as I wanted to test each separately to give us wiggle room in the future. Next I installed certbot with snap: sudo snap install --classic certbot. Certbot is the command line tool used by LetsEncrypt to establish ownership of domains to grant TLS certificates. Once all the software was installed I tested the domain by typing teambellevue.xyz into my search bar. Initially no result came through due to my security Firefox plugins. Once I allowed HTTP traffic the default apache page loaded without issue.\nTLS Now that I had confirmed HTTP traffic worked, I now needed to allow HTTPS traffic. First I tested certbot\u0026rsquo;s installation with sudo ln -s /snap/bin/certbot /usr/bin/certbot. According to the documentation this command tests to see if certbot is in working condition for granting certificates. Next I ran sudo certbot --apache which both configured my apache installation to work with certbot and ran the required scripts to grant the domain a certificate. I entered the Domain name and after a short while, the command returned a success message. I next tested the domain again in a web browser, this time without disabling HTTPS security plugin, and it connected immediately with the desired protocol. Next I entered sudo certbot renew --dry-run This command simulates the certificate running out and requests another from the certificate authority. I wanted to test this now so we wouldn\u0026rsquo;t run into issue after the process was setup. It worked first try so I knew in the future we need not worry about our certificate running out. Next I uninstalled Apache and tried every step over again with Nginx. The steps proceeded just as smoothly so I knew moving forward, we could pick either for our project.\nConclusion and Considerations This was my real first outing in HTTPS and I was shocked with how simple it was. In the past I have hosted standard HTTP-only websites and shied away from certificates because I thought it would be too much of a time sink. However Let\u0026rsquo;s Encrypt and Certbot really simplified the process, and even for free. Moving forward I need to look into a way of automating this process. Perhaps at first I could make it into a simple bash script to see if I could run it without user input, and later I can progress into using Ansible for use with multiple web servers at once. However, I am already pleased with my current progress, and that is a project for a different blog.\n","date":"2021-03-03","permalink":"https://robertfleming448.github.io/post/481blog2/","tags":["CIT-481","Group Project","Apache","Nginx","TLS"],"title":"CIT 481 blog 2"},{"content":"Introduction Despite this being my senior year, and having a relatively light scholastic work-load, I still thankfully have something to write about. I have a backlog of projects that I can dive into should the need arise later in the semester perhaps when things slow down. Additionally, only 2 of my 3 major classes this semester I would consider \u0026ldquo;technical.\u0026rdquo; One of those has little to no coding on a weekly basis. However this week that class has provided me with an opportunity to write down my lab progress. We were tasked with configuring, building, and installing a custom kernel for FreeBSD.\nFreeBSD FreeBSD is a modern OS that operates very closely to a UNIX environment. Its default shell is the Bourne Shell, sh, and as a Linux user I am very familiar with its successor the Bourne Again Shell, bash. As a result, I will not go over the installation or base configuration of FreeBSD but I will dive straight into the Kernel build.\nAssignment Overview The Assignment\u0026rsquo;s goal was rather straightforward, discover the mandatory boot devices and configure a Kernel to only use those devices at startup to speed up the boot process. The professor provided us the manual pages for DMESG and PCICONF. Using these commands we be able to complete the tasks he assigned.\nConfiguration In order to configure a custom Kernel, we required a base, or a GENERIC file. These generic files can be used as starter files for Kernels based on types of CPU architectures. In order to find our GENERIC we had to look in the /usr/src/sys/ directory. In that directory we needed to choose the CPU architecture that matched our machine. As a result I chose amd64/. From there I went to the conf/ directory where I found the GENERIC.\nPrerequisites to Editing I very well could have used vi or nano to edit the config file, but I wanted a more familiar environment. Additionally, I knew that I needed to be able to export my work from the machine eventually so I chose to get it out of the way now. There were 2 options for saving work to my host OS either:\n Sharing a directory between host and guest OS\u0026rsquo;s via VMware Connecting removable drive and transferring it between host and guest  I chose the second option as I was unaware if the Guest Additions Tools would work on FreeBSD, and I knew mounting a drive would take very little time. I also saved the output of DMESG and PCICONF to a text file so I could further explore the output on my host machine. Once I mounted the drive, I copied the GENERIC and text files to it and unmounted to further edit on my host OS.\nEditing the Generic To edit the generic I went through the devices loaded, seen in DMESG, and commented out missing devices in the GENERIC. While this did work it was not without a little trial an error. Sometimes I would comment out a device that despite not being in the DMESG load order, had a vital dependency for something else. I kept editing back in fourth in preparation for the next step.\nHere is a segment of my edited config file: Note how the only used SCSI controller is not commented # SCSI Controllers device\tahc\t# AHA2940 and onboard AIC7xxx devices #device\tahd\t# AHA39320/29320 and onboard AIC79xx devices #device\tesp\t# AMD Am53C974 (Tekram DC-390(T)) #device\thptiop\t# Highpoint RocketRaid 3xxx series #device\tisp\t# Qlogic family #device\tispfw\t# Firmware for QLogic HBAs- normally a module #device\tmpt\t# LSI-Logic MPT-Fusion #device\tmps\t# LSI-Logic MPT-Fusion 2 #device\tmpr\t# LSI-Logic MPT-Fusion 3 #device\tncr\t# NCR/Symbios Logic #device\tsym\t# NCR/Symbios Logic (newer chipsets + those of `ncr\u0026#39;) #device\ttrm\t# Tekram DC395U/UW/F DC315U adapters #device\tisci\t# Intel C600 SAS controller #device\tocs_fc\t# Emulex FC adapters # ATA/SCSI peripherals device\tscbus\t# SCSI bus (required for ATA/SCSI) #device\tch\t# SCSI media changers device\tda\t# Direct Access (disks) #device\tsa\t# Sequential Access (tape etc) device\tcd\t# CD #device\tpass\t# Passthrough device (direct ATA/SCSI access) #device\tses\t# Enclosure Services (SES and SAF-TE) #device\tctl\t# CAM Target Layer\nBuilding the Kernel Once I finished editing my Kernel for the first time, I remounted the device and copied my edited config file back into the /usr/src/sys/amd64/confdirectory. After that I navigated to the /usr/src directory and typed the following command to attempt a build of my Kernel: make buildkernel KERNCONF=KERNEL_CUSTOM This would run for 5-6 minutes than reach a fatal error. From looking at the error messages, I could glean what dependency was missing in my Kernel device list. I would edit this progressively to re-add the required devices. After 5 or so attempts the Kernel was built unsuccessfully with zero warnings or errors.\nInstalling the Kernel After the successful build I ran make installkernel KERNCONF=KERNEL_CUSTOM. This took a few minutes but it seemed to work first try. The only thing I had left to do was reboot the machine to see if it broke or not. After a reboot and a successful sign-in, I concluded that my efforts were successful, and I prepared my lab submission consisting of my modifed GENERIC as well as text file containing the new outputs of DMESG and PCICONF.\nConclusion While this lab did take longer than I expected, I did finish well before the deadline so I can possible make further adjustments to my Kernel before submission. I think this lab was a good test of troubleshooting as I really had to explore the documentation and forums in order to diagnose the dependency errors I faced during building. Hopefully in the future I can report back on my developments in FreeBSD labs, but that does it for now.\n","date":"2021-02-24","permalink":"https://robertfleming448.github.io/post/481blog1/","tags":["CIT-481","FreeBSD","Unix"],"title":"CIT 481 blog 1"},{"content":"Introduction As we start a fresh semester, I find a fresh set of visuals will help keep things interested. As such, my blogs now have a sleek minimalistic design using the Hugo Fuji theme. Yes you read that right. My blog will now be using Hugo. For my previous blogs that I wrote during my semester in CIT 480, I used a similar static site generator called Jekyll. While it did the job, I by no means used all its features to their best extent, and I overall was not too fond of it. That is why I decided to use Hugo from this point forward. To practice my new static site generator I decided to convert my Jekyll blogs into the slightly different Hugo Format. In this blog I will go over the installation steps and the problems I faced converting my prior blog entries.\nInstallation Compared to Jekyll, Hugo was much easier to work with. Unlike Jekyll, Hugo is precompiled. This means that all I have to do is download the executable and run it to generate static sites. I downloaded Hugo from their GitHub repository, and added a new install directory in my C: drive. Once downloaded I made bin directory and extracted the executable to the location. Next I tested if Hugo worked from my PowerShell by typing Hugo version while in the same directory. The shell outputted the version I downloaded so I knew it was a success.\nConfiguration Now that I have confirmed that Hugo works properly, I wanted to be able to use Hugo in any directory, not just the bin folder in my Hugo directory. In order to do this I had to edit my Windows Path Environment Variable. In my systems Properties window which I accessed from my Control Panel, I navigated to the Environment Variables section and scrolled to the path variable. At the end of the list, I added C:\\hugo\\bin so my shell would know where to look when I inputted future Hugo commands. After testing in a different directory, I was able to finally get to writing the blog itself.\nHugo Blog Configuration The first thing I wanted to do was to find and simple elegant theme. To do this I went to themes.gohugo.io. I chose the Fuji theme as it was much less busy than the Jekyll theme I used last semester. I added the git submodule to my blog repository with git submodule add https://github.com/amzrk2/hugo-theme-fuji.git themes/fuji and used their default config file as a base for my config.toml I edited a few things, namely the title of the blog and some links in the sidebar.\nWriting a Hugo Blog This step was the part that made me decide to make the swap over to Hugo. In the previous semester, I tried unsuccessfully to install Jekyll on my Windows 10 Pro Desktop computer. As a result, all configuration was done on my Ubuntu laptop environment. While I am plenty comfortable on Ubuntu and other Linux environments, I am partial to Windows and all the configurations I\u0026rsquo;ve made over the years.\nPrevious Mistakes I mentioned prior that I was not all that fond of Jekyll but this was likely my mistake. To avoid spending more time in my Linux environment, I pre-generated all the blogs ahead of time, so that I could edit them on my Windows Desktop without requiring a Jekyll Installation. There were several problems with that:\n I could not see my blog till I pushed to a Repository Error messages would not be shown to me if the page did not build incorrectly Editing the dates every week was widely inefficient.  This led to another unique problem for my Jekyll blog; The index page did not generate properly. While the blogs existed and were perfectly accessible via link, navigating through them would be difficult. To fix this I regrettably wrote an index page from scratch, somewhat defeating the purpose of a static site generator. However as I am learning Hugo I am attempting to right those wrongs.\nNew Additions When I went back and updated my blogs I decided to change my headings structure. In my previous blogs I manually inserted the heading HTML tags. This disrupted the flow of the page, and required me to manually place \u0026lt;br\u0026gt;\u0026rsquo;s to fix my spacing mistakes. I am now using the proper markup heading syntax consisting of ## for headings and ### for subheadings. An upside of this format is the nice table of contents that is automatically generated to the right. I also decided to tag blogs with relevant info so navigating through specific topics would be easier.\nFormat changes While the new heading syntax was just a quality of life improvement, here were a few format changes that were required to prevent breaking the page. Hugo has a slightly different structure for code snippets then Jekyll. Jekyll used {% highlight [LANGUAGE] %} where Hugo uses {{\u0026lt; highlight [LANGUAGE] \u0026gt;}}. This could be changed using a couple replace filters in my IDE. Beyond that, I was able to fully copy and paste my previous blogs into the new format. I was pleased to discover this also worked for my previous blogs included images as well.\nIssues Since this was my first time using Hugo, I did run into a few problems. From my understanding, unlike Jekyll, Hugo isn\u0026rsquo;t natively supported by GitHub pages. By this I mean, it will not generate the static site automatically upon pushing to a repository. I think this because in GitHub\u0026rsquo;s repository settings it lists Jekyll themes but not Hugo themes. This took me while to discover, but to solve it I just created 2 repositories. One would hold all my markup code and configuration files, and the other would be the public folder generated by Hugo when the hugo command is run. This command will build a static site in HTML from the markup you have written. After a few issues with base domain configuration in my config.toml file, the blog was up with the theme intact.\nConclusion Overall I found this process very gratifying. Jekyll never quite cliqued for me, and I found its setup steps laborious. Hugo seems much more straightforward, while allowing me to remain the familiarity of the markup I\u0026rsquo;ve already learned. Moving forward, making weekly blogs will be much more streamlined thanks to automatic index page being generated, and the ability to build right from my preferred environment.\n","date":"2021-02-18","permalink":"https://robertfleming448.github.io/post/481blog0/","tags":["CIT-481","git","Hugo"],"title":"CIT 481 blog 0"},{"content":"Introduction This blog will be the final part in my PHP learning series. In the last part we went over the basic scripts and functions required to create a basic website login/signup feature. In this section I will go over the additional optional features I added to make the website more complete. The two features I will go over in this blog are the change password script, and the integration of 2 step authentication via Twilio webservices. Beginning Setup\nBefore we dive straight into the scripts first, we need to a page to host links to these scripts. Since I do not feel these features should be included as default entries on the standard account page, I created a separate options php page. This page is very basic and like the other pages use the same universal header and footer. It only exists to hold the forms required to change user settings. Change Password\nThe first feature is the change password feature. For any website it is wise to allow users to change passwords should their old choice become compromised. So, while I do consider this an optional feature it’s one of the more “required” optional features. This script functions very similarly to the login script so much of the code maintains that structure. For more information on that script, visit my 11th blog in this series. This script is similar because it takes the users old password, confirms it is correct, and will change to the newly inputted password. Additionally, similarly to the signup script, a user must enter the same password twice to ensure no mistakes have been made in the entry. Like last time, there are error handlers that prevent users from typing invalid inputs for example:\n//error handler receivers \tif(isset($_GET[\u0026#34;error\u0026#34;])){ //Error handlers that call functions to check errors, will print corresponding problem under form \tif($_GET[\u0026#34;error\u0026#34;] == \u0026#34;emptyinput\u0026#34;){ echo \u0026#34;\u0026lt;p\u0026gt;Please fill all fields\u0026lt;/p\u0026gt;\u0026#34;; } else if($_GET[\u0026#34;error\u0026#34;] == \u0026#34;pwdMatch\u0026#34;){ echo \u0026#34;\u0026lt;p\u0026gt;Passwords don\u0026#39;t match\u0026lt;/p\u0026gt;\u0026#34;; } else if($_GET[\u0026#34;error\u0026#34;] == \u0026#34;wrongPwd\u0026#34;){ echo \u0026#34;\u0026lt;p\u0026gt;Incorrect Password\u0026lt;/p\u0026gt;\u0026#34;; } else if($_GET[\u0026#34;error\u0026#34;] == \u0026#34;stmtFailed\u0026#34;){ echo \u0026#34;\u0026lt;p\u0026gt;Sorry Something went wrong. Please try again later.\u0026lt;/p\u0026gt;\u0026#34;; } else if($_GET[\u0026#34;error\u0026#34;] == \u0026#34;success\u0026#34;){ echo \u0026#34;\u0026lt;p\u0026gt;Password changed Succesfully\u0026lt;/p\u0026gt;\u0026#34;; } } //error handler functions example: \tfunction emptyInputChange($oldPwd, $pwd, $pwdRepeat) //checks if change password fields are empty, if empty return true \t{ $result; if(empty($oldPwd) || empty($pwd) || empty($pwdRepeat)){ $result=true; } else { $result=false; } return $result; }  If the password matches what we have recorded in our database, and no error handlers return true, we can change the user’s password and notify them of the change. The sql statement script is located here:\nfunction changePassword($conn, $userUid, $pwd) //changes user password by altering entry in DB { $sql = \u0026#34;UPDATE users SET usersPwd=? WHERE usersUid=? OR usersEmail=?;\u0026#34;; $stmt = mysqli_stmt_init($conn); //prepared statement to prevent sql injection attacks  if (!mysqli_stmt_prepare($stmt, $sql)) { header(\u0026#34;location: ../accountSettings.php?error=stmtFailed\u0026#34;); exit(); } $hashedPwd = password_hash($pwd, PASSWORD_DEFAULT); mysqli_stmt_bind_param($stmt, \u0026#34;sss\u0026#34;, $hashedPwd, $userUid, $userUid); mysqli_stmt_execute($stmt); mysqli_stmt_close($stmt); header(\u0026#34;location: ../accountSettings.php?error=success\u0026#34;); }  Like last time we ensure to hash the password so even in the event of data breach user password information will not be divulged.\nWhile this step, seems simple, it actually gave me the most difficulty of any of the scripts. I attribute this to the fact that this was my first script written without aid from an online source or tutorial. These scripts helped me make errors throughout the process and learn the language better.\nTwo Factor While I mentioned that the prior feature gave me more trouble, it is not to undermine the effort that went into getting 2-factor authentication to work. This process was much longer and for the sake of brevity, I will only mention the key aspects that were not repeated.\nFirst off, was to figure out how I was going to send a SMS message for 2 factor purposes. If you recall in my first blog in this series, I went over a bot program I made to aid in registration. In that program I used the java SDK for Twilio SMS web services. So, for this section since I am using PHP I chose to dabble in the PHP SDK Twilio provides. If you want more detail on the Twilio service on not just the application for this project see my first blog.\nThe scripts required for this are the following:\n enable 2-factor disable 2-factor check 2-factor on enable check 2-factor on login  I chose to differentiate the last 2 so that a user could input a different phone number on enabling in case they were to make a mistake typing their phone number. This could have been done with a few conditional statements to make only one page/script, but it seemed far too complicated and I would be able to save time just reusing code snippets.\nThis is the code for enabling 2-factor:\nfunction disable2Factor($conn, $username) //disables 2-factor by setting user phone number to null in DB entry \t{ $sql = \u0026#34;UPDATE users SET usersPhone=NULL WHERE usersUid=? OR usersEmail=?;\u0026#34;; $stmt = mysqli_stmt_init($conn); //prepared statement to prevent sql injection attacks  if (!mysqli_stmt_prepare($stmt, $sql)) { //echo(\u0026#34;Error description: \u0026#34; . $conn -\u0026gt; error); \theader(\u0026#34;location: ../accountSettings.php?error=stmtFailed\u0026#34;); exit(); } mysqli_stmt_bind_param($stmt, \u0026#34;ss\u0026#34;, $username, $username); mysqli_stmt_execute($stmt); mysqli_stmt_close($stmt); $_SESSION[\u0026#34;TwoFactor\u0026#34;] = false; header(\u0026#34;location: ../accountSettings.php?error=twoFactorDisabled\u0026#34;); }  \u0026hellip;and this is the code for disabling 2-factor\nfunction disable2Factor($conn, $username) //disables 2-factor by setting user phone number to null in DB entry \t{ $sql = \u0026#34;UPDATE users SET usersPhone=NULL WHERE usersUid=? OR usersEmail=?;\u0026#34;; $stmt = mysqli_stmt_init($conn); //prepared statement to prevent sql injection attacks  if (!mysqli_stmt_prepare($stmt, $sql)) { //echo(\u0026#34;Error description: \u0026#34; . $conn -\u0026gt; error); \theader(\u0026#34;location: ../accountSettings.php?error=stmtFailed\u0026#34;); exit(); } mysqli_stmt_bind_param($stmt, \u0026#34;ss\u0026#34;, $username, $username); mysqli_stmt_execute($stmt); mysqli_stmt_close($stmt); $_SESSION[\u0026#34;TwoFactor\u0026#34;] = false; header(\u0026#34;location: ../accountSettings.php?error=twoFactorDisabled\u0026#34;); }  These functions work by inserting the phone number into the user’s database entry to set 2-factor and setting the value to null to deactivate. During login we can check if this value is set or not to determine whether the user should receive the 2-factor prompt.\nLastly we need to generate the code for the user to enter into the prompt:\nfunction codeGen() //generates secure code from 000000 to 999999 \t{ $min=0; $max=999999; $number= random_int($min , $max ); $number=str_pad($number, 6, \u0026#39;0\u0026#39;, STR_PAD_LEFT); return $number; }  This simple function produces a secure random integer from 0-999999 padding the front with zeros in order to reach six full digits. This code is then sent to the user via the Twilio sendSMS function, where the user can input the number to log themselves in.\nConclusion I hope this summary of my additional PHP scripts/features has been informative and/or entertaining. I was very pleased to get 2-factor working in particular because to me it is an impressive display for a student of web development to include. I hope the professor feels this way as well. Since this is my last blog as of now, I cannot confirm the contents of my future endeavors. However, if I were to take a guess it would involve my usage of D3.js framework and my use of JavaScript in the project. Thank you for taking the time to read these blogs, they have been useful as a method of gathering my thoughts while also making sure my code is halfway presentable.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog12/","tags":["CIT-480","PHP","HTML"],"title":"CIT 480 blog 12"},{"content":"Introduction This blog will feature part 2 of my PHP login system explanation. For some more background make sure to check out my 10th blog in this series. This blog will be rather lengthy so I will try my best not repeat my past blog’s contents. Without further ado, this is how I made a login system for my Comp 484 final group project.\nRequired Scripts This system required many scripts written in PHP. PHP is a server-side scripting language. In the past I have done server-side scripting via CGI scripting but never have I used PHP. So, if some of my explanations aren’t 100% accurate I apologize ahead of time. Some of the required scripts we need are:\n signup script login script logout script  Additionally, in order to store user information we will need to use a database. For this project I chose a MySQL database that came bundled with a Windows server client package called XAMPP. XAMPP’s SQL database is MariaDB which is a fork of MySQL and as a result has interchangeable syntax. I chose this to software to make development easier on my localhost while making the application easy to transfer to other operating systems. To interact with the database I also utilized a database connection handler script.\nSignup Script The signup script is ran whenever a user on the signup.php page fills out the signup form. The form contains standard information such as Username, Email, password, and repeat password. Once the user presses the submit button, error handlers make sure the information is valid through multiple functions. Here is an example of one of these error handlers and functions: if (emptyInputSignup($name, $email, $username, $pwd, $pwdRepeat) !== false) { //error handlers  header(\u0026#34;location: ../signup.php?error=emptyinput\u0026#34;); exit(); } function emptyInputSignup($name, $email, $username, $pwd, $pwdRepeat) //checks if fields are empty on signup page; returns true if fields are empty { $result; if(empty($name) || empty($email) || empty($username) || empty($pwd) || empty($pwdRepeat)){ $result=true; } else { $result=false; } return $result; } \nIf the script is interrupted by an error, the user is redirected to the signup page with a query string that will inform the PHP script to update the page in a manor to alert the user of the error that has occurred. If no such error occurs, the script will progress to the createUser function. This function will take the information from the form and will interact with the database Handler script to insert a new entry to the database on the server. In this function there is something called an SQL statement. This statement will ensure that no SQL injection attacks may occur by validating a command before data is added to it. The statement for the createUSer function is below: $sql = \u0026#34;INSERT INTO users (usersName, usersEmail, usersUid, usersPwd) VALUES (?, ?, ?, ?);\u0026#34;; $stmt = mysqli_stmt_init($conn); //prepared statement to prevent sql injection attacks  if (!mysqli_stmt_prepare($stmt, $sql)) { header(\u0026#34;location: ../signup.php?error=stmtFailed\u0026#34;); exit(); } $hashedPwd = password_hash($pwd, PASSWORD_DEFAULT); mysqli_stmt_bind_param($stmt, \u0026#34;ssss\u0026#34;, $name, $email, $username, $hashedPwd); mysqli_stmt_execute($stmt); mysqli_stmt_close($stmt); \nThis code will communicate with the database and if the connection is successful, will add the user’s name, username, email address, and hashed password to the database. I could not call myself an IT student if I stored the user’s password in plain text, could I?\nLogin Script Now that a user can register for the site, they need to be able to login with the credentials they set. This is remarkably similar to the prior step but in reverse order almost. Users must still enter their credentials without error, however this time, the script retrieves info from the database to confirm if credentials are correct or not. This is accomplished with an SQL statement as well:\n$stmt = mysqli_stmt_init($conn); //prepared statement to prevent sql injection attacks  if (!mysqli_stmt_prepare($stmt, $sql)) { header(\u0026#34;location: ../signup.php?error=stmtFailed\u0026#34;); } mysqli_stmt_bind_param($stmt, \u0026#34;ss\u0026#34;, $username, $email); mysqli_stmt_execute($stmt); $resultData = mysqli_stmt_get_result($stmt);  If users inputted credentials match credentials of the database, a session is started and session variables containing key user information is saved to the browser. By saving these session variables, we can dynamically change the page to adhere to a particular user’s database entry. For example, now that a user is logged in, we no longer need to display the login or signup page on the navbar. Additionally, we can replace these options with other links such as an account page or a logout option.\nLogout Script Compared to the other scripts in this blog, this is by far the simplest. The login script involved saving user information to session variables to change page content via PHP. This script must simply restore it to that previous state. This can be done by clearing all set session variables and by closing the session. Below is a simple script that accomplishes this task.\n\u0026lt;?php //logs out user by destroying session information. Redirects to main page session_start(); session_unset(); session_destroy(); header(\u0026#34;location: ../index.php\u0026#34;);  Acknowledgements I would like to take this part to acknowledge the person that taught me the basics of PHP used in these scripts and many more. Had it not been for Dano Krossing’s PHP tutorial series on YouTube I would have been very lost. He truly paved the groundwork that would allow my team and I to integrate login features to our website and I wanted to give credit where credit is due. As a result of me following this tutorial some of the code shown in this blog will look like code displayed in his video. I have added to the code and suited it to better fit project guidelines but that falls beyond the scope of this blog.\nConclusion Overall, this step took a very long time. I had never worked with PHP before, but now that I am more experienced with it, I can more easily write code by myself and read online examples. I think my ability to adapt to this language was due to its relative similarity to Java, of which I am very familiar with. In the next blog I will go over some additional modules I added to my PHP login system, without the use of any online guide or tutorial. This will include a change password system and a 2 factor authentication check using Twilio infrastructure.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog11/","tags":["CIT-480","PHP","HTML"],"title":"CIT 480 blog 11"},{"content":"Introduction For the next few blogs, I will be writing a series showing off bits of code I have written for my Computer Science 484 final group project. This blog will be used to provide a little background and explain a program that I wrote to help us in the future.\nThe Project The project is an interactive web application that uses templates to auto-generate hundreds of unique algebra problems focusing on middle-school and high-school students. The application will track user progress by recording how many correct and incorrect answers a user gets on specific subjects. To implement this several systems needed to be made.\n A login system where users can access their statistics A database to house user information and statistics A script system to generate problems and receive user input  For this project we decided the best server-side scripting language to accomplish the login system would be PHP. While I have had more experience in the past with bash CGI scripting after looking at PHP formatting and use-cases, I determined it a far better fit for this project. Additionally, the database we will be using is a MySQL database which functions well with PHP. Lastly for the client-side scripting we will be using JavaScript.\nToday’s Subject As I mentioned previously to auto generate these problems, we will be using templates. To elaborate on this, we will create the skeleton of the problem \u0026lt;val\u0026gt;y=\u0026lt;val\u0026gt;x+\u0026lt;val\u0026gt; and substitute randomly generated numbers into the skeleton. By solving the skeleton ahead of time, we can provide better instructions for potential users while still maintaining a wide variety of problems. Now the problem we have is the formatting these skeletons. To demonstrate I’ll provide one of the templates that has already been made:\nlet answer = [x,\u0026#34;x\u0026#34;,\u0026#34;=\u0026#34;,y,\u0026#34;y\u0026#34;,\u0026#34;+\u0026#34;,b]; document.getElementById(\u0026#34;section2\u0026#34;).innerHTML = (toString(answer)); answer = [0,\u0026#34;=\u0026#34;,y,\u0026#34;y\u0026#34;,\u0026#34;+\u0026#34;,b,\u0026#34;-\u0026#34;,x,\u0026#34;x\u0026#34;]; document.getElementById(\u0026#34;section3\u0026#34;).innerHTML = (toString(answer)); answer = [\u0026#34;-\u0026#34;,y,\u0026#34;y\u0026#34;,\u0026#34;=\u0026#34;,\u0026#34;-\u0026#34;,x,\u0026#34;x\u0026#34;,\u0026#34;+\u0026#34;,b]; document.getElementById(\u0026#34;section4\u0026#34;).innerHTML = (toString(answer)); answer = [\u0026#34;y\u0026#34;,\u0026#34;=\u0026#34;,x/y,\u0026#34;x\u0026#34;,\u0026#34;+\u0026#34;,b/y]; document.getElementById(\u0026#34;section5\u0026#34;).innerHTML = (toString(answer));  While the formatting on this isn’t overly complicated it still takes some time to write out the entire thing in JavaScript array format. This format was chosen so we would have easier access to each individual item in the array for editing. To simplify this process for future templates, I decided to create a program in Java, as it is the language, I am most familiar with.\nThe Program: Like I always do I mentally list out the requirements for a program so I can break each function down. For this program we would need:\n Something to accept user input Something to determine what each part of the input is e.g. alphabet, number, or symbol Something to format it in JavaScript array notation  At first I used the Java Scanner class located in the util package, but later I decided to use JOptionPane to make the program more accessible to the other collaborators on the project. Using the showInputDialog() method I was able to get user input as a String. Next using an ArrayList to make the array data more manageable and a for loop, I broke the string into a list of individual characters. Next, I put each character through a distinguisher method to determine if they were an alphabetical char or not. I chose this method as numbers and symbols would be used for substitution and problem structure respectively, however characters were meant to be literal. Based on this distinguisher I would format the alphabetic characters and symbols as strings while the numbers would be left for future replacement by a JavaScript function. Lastly, I would assemble this into an array and print each step in the problem in one large output to be copied and pasted into the JavaScript template script. The code for the program is bellow: package main; import java.awt.Component; import java.util.*; import javax.swing.*; public class TemplateHelper { private static final Component frame = null; public static void main(String[]args) { ArrayList\u0026lt;String\u0026gt; problem = new ArrayList\u0026lt;String\u0026gt;(); JOptionPane.showMessageDialog(frame, \u0026#34;Type in your equation to format. Use numerals (1,2,3...) for numerals and alpha (a,b,c...) for variables.\u0026#34;); System.out.println(\u0026#34;Type in your equation to format. Use numerals (1,2,3...) for numerals and alpha (a,b,c...) for variables.\u0026#34;); int x=0; while(true) { x++; System.out.println(\u0026#34;Enter your equation or enter ~ to start new or Enter ! to stop.\u0026#34;); System.out.print(\u0026#34;\u0026gt;\u0026#34;); String input=JOptionPane.showInputDialog(\u0026#34;Enter your equation or enter ~ to start new or Enter ! to stop.\u0026#34;); if(input==null||input.equals(\u0026#34;!\u0026#34;)) { break; } if(input.equals(\u0026#34;~\u0026#34;)) { x=0; htmlFormat(problem.toArray()); problem.clear(); } else { System.out.print(\u0026#34;Step \u0026#34;+x+\u0026#34; is: \u0026#34;); System.out.println(format(input)); problem.add(format(input)); } } htmlFormat(problem.toArray()); System.out.println(\u0026#34;Execution Finished\u0026#34;); } public static String format(String input) { char[] result = new char[input.length()]; ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;(); for(int i=0; i \u0026lt; input.length(); i++) { result[i]=input.charAt(i); } for(int i=0; i \u0026lt; result.length; i++) { if(isAlpha(result[i])==false) { if(Character.isDigit(result[i])==false) { list.add(\u0026#34;\\\u0026#34;\u0026#34;+result[i]+\u0026#34;\\\u0026#34;\u0026#34;); } else { list.add(Character.toString(result[i])); } } else { list.add(\u0026#34;\\\u0026#34;\u0026#34;+result[i]+\u0026#34;\\\u0026#34;\u0026#34;); } } return list.toString(); } public static boolean isAlpha(char input) { if(Character.isLetter(input)) { return true; } else { return false; } } public static String toString(char [] input) { String result = null; return result; } public static void htmlFormat(Object[] objects) { if(objects.length==0) { return; } System.out.println(\u0026#34;Problem Finished HTML/Javascript code output: \\n\u0026#34;); String output=\u0026#34;\u0026#34;; for(int i=0; i\u0026lt;objects.length; i++) { if(i==0) { System.out.print(\u0026#34;let answer = \u0026#34;); output+=(\u0026#34;let answer = \u0026#34;); } else { System.out.print(\u0026#34;answer = \u0026#34;); output+=(\u0026#34;answer = \u0026#34;); } System.out.println(objects[i]+\u0026#34;;\u0026#34;); output+=(objects[i]+\u0026#34;;\\n\u0026#34;); int count = i+2; System.out.println(\u0026#34;document.getElementById(\\\u0026#34;section\u0026#34;+ count +\u0026#34;\\\u0026#34;).innerHTML = (toString(answer));\u0026#34;); output+=(\u0026#34;document.getElementById(\\\u0026#34;section\u0026#34;+ count +\u0026#34;\\\u0026#34;).innerHTML = (toString(answer));\\n\u0026#34;); System.out.println(\u0026#34;\u0026#34;); output+=\u0026#34;\\n\u0026#34;; } new PaneMessage(output); } } package main; import java.awt.EventQueue; import javax.swing.JOptionPane; import javax.swing.JScrollPane; import javax.swing.JTextArea; import javax.swing.UIManager; import javax.swing.UnsupportedLookAndFeelException; public class PaneMessage { public PaneMessage(String message) { EventQueue.invokeLater(new Runnable() { @Override public void run() { try { UIManager.setLookAndFeel(UIManager.getSystemLookAndFeelClassName()); } catch (ClassNotFoundException | InstantiationException | IllegalAccessException | UnsupportedLookAndFeelException ex) { } JTextArea ta = new JTextArea(30, 100); ta.setText(message); ta.setWrapStyleWord(true); ta.setLineWrap(true); ta.setCaretPosition(0); ta.setEditable(false); JOptionPane.showMessageDialog(null, new JScrollPane(ta), \u0026#34;RESULT\u0026#34;, JOptionPane.INFORMATION_MESSAGE); } }); } }\nPlease note that some of the code in this exists from a previous command line only version, thus explaining the print statements.\nResults: Below I will post some of the resulting screenshots of my code to demonstrate how the process would look from start to finish:\nRunning the .jar Write out each problem step by step Finished Result Copy and Paste into Javascript Reformatted and substituted numbers for values Template in action (Beta Styling) Conclusion In the end I am happy with the program and my group appears to be so too. I’m hoping the time I invested on this will make our future template creating efforts easier. Next blog I will talk about how I created a PHP login system from scratch ...with the massive help of an online tutorial Please stay tuned for the next in the Comp 484 web application series!\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog10/","tags":["CIT-480","HTML","JavaScript","Java"],"title":"CIT 480 blog 10"},{"content":"Introduction This blog will detail the process I used to complete the fourth lab for CIT 480, in which I configured an AWS EC2 instance to allow communication with an AWS simple storage system bucket. For the most part I thought this lab was pretty straight forward but I did have a minor hiccup which I will detail in a later section.\nBackground I currently have access to 2 AWS accounts. One is my educate account which still has most of the initial 100 dollar funds. The other is my regular AWS account, which although it does not have any education credits is still within its, first year granting me free tier access to resources. As I think AWS knowledge will be a real boon to my career prospects, I have chosen to use the standard environment even if it means I end up paying for some of the resources I use. By doing it this way, I feel I will the most genuine experience with AWS and be more familiarized with how a traditional account functions.\nCreating the EC2 This step is the part I am most familiar with. Luckily, I have completed the previous Ansible task with EC2 instances so I am very comfortable with the process. I chose an Ubuntu image as it is my Linux distribution of choice and chose to spin up a t2.micro instance to minimize resource costs. Lastly, I changed the security group do be one I have used in the past. This group allows http traffic everywhere yet restricts SSH traffic to exclusively my home IP range. After I selected the already generated key-pair to use, the instance began initialization. I ran the ssh -i ~/Downloads/\u0026lt;key_name\u0026gt;.pem ubuntu@\u0026lt;EC2_IP\u0026gt; to communicate with the instance and to update the apt repository for future use. I would also install Apache and update the required files at this time. I did this with apt-get install apache2 and by using the vim text editor to overwrite the default apache page and styling.\nCreating the S3 Bucket This step I had less experience with but I felt confident in my abilities due to the previous in class live demo. First, I went to the S3 section of the AWS console and selected the create bucket option. Next, I gave my bucket an appropriate name and left all the other settings default. After that, I selected the create bucket option and the resources were allocated for my usage.\nCreating the IAM Role I had previously created an IAM user so I was somewhat familiarized with the IAM concept and security policies already. For that reason, I was confident this step would cause little issue. I went to the IAM roles section and selected the create role option. From there I specified that it would be used for an AWS service and clicked the EC2 common use case option. Next, I gave the role full bucket permissions with the  AmazonS3FullAccess policy. I left every other option as default and the role was created successfully.\nGiving the EC2 the Role I returned to the EC2 service dashboard and navigated to actions menu. From there I was able to change the IAM settings for the selected instance and give it the role I had just created. Next as a precaution, I disconnected my SSH connection to the machine and restarted the instance. Here comes the “hiccup” I had mentioned earlier. When I restarted the instance, I had not noticed that the IP address I was assigned had changed. In retrospect this makes total sense, since I was not reserving the address itself. I figured out the issue when trying to SSH and corrected the problem in my SSH command and my apache index page to reflect the IP change.\nConnecting the EC2 Instance to the S3 Bucket First, I installed the awscli for ubuntu with sudo apt install awscli After confirming the installation was a success, I researched the commands required to perform functions with the s3 bucket. In order to list the available buckets, I used the aws s3api list-buckets command which returned the information in a handy JSON format. Next in order to copy my index.html page to the bucket I used the aws s3 cp index.html s3://\u0026lt;bucket_name\u0026gt;/ command. I checked in the AWS console to confirm the transfer was a success and did register a file had been transferred. Lastly to confirm the file with my cli I used the aws s3 ls s3://\u0026lt;bucket_name\u0026gt;/ command. This command listed all the files located on the root of the bucket. The command outputted the file that I had just uploaded. After confirming the Apache webpage was loaded correctly and taking my requisite screen captures, I exited my SSH connection and deleted the contents of the S3 bucket. Once it was emptied, I terminated the bucket and EC2 instance to prevent further charges.\nConclusion Overall, this task I found to be rather straight-forward. I am glad I had the previous experience with EC2 to speed up the initial process. While I didn’t struggle much with this lab, I still feel that I gained some valuable information. I learned more about the usefulness of IAM roles, and about how AWS services can be interconnected to make more complex web applications.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog9/","tags":["CIT-480","AWS"],"title":"CIT 480 blog 9"},{"content":"Introduction In this blog I will talk about all the methods I’ve used to configure my network in order to play multiplayer games with friends. I thought this would be a good addition to a blog I posted two weeks ago as it has similar concepts, where I discussed troubleshooting connection problems with my friends trying to play a multiplayer game. For this blog I will be focusing on the four methods I’ve used to play the popular sandbox game: Minecraft with my friends in the past.\nOption 1: The Easiest Yet Least Convenient By far the easiest way to play most PC games with friends is to actually do no network configuration. By that I mean a LAN party. Depending on the application connectivity can be done in two ways. Sometimes the game or software is smart and will allow clients to search the network for active games. This is the case with Minecraft. For other games, all you must do is find out what the private IP address is for the machine hosting the session. This can be done easily in Windows by opening a command prompt and typing ipconfig. I have done this with my friends in the past but it is certainly a hassle. For one everybody needs to bring their computers to one local area network and connect to a network. Additionally, you need to make sure there is enough ethernet cables to go around or else you will have to connect wirelessly which seriously degrades the experience by introducing latency issues.\nOption 2: Easy but not Convenient This option acts as sort of an intermediary between the prior method and the next. For Minecraft in particular it requires the host to run the server-side executable jar file available at Minecraft.net. This will allow multiple users to play in the same session when not on the same LAN. This method is useful when the users do not have access to edit router port triggering settings. Next both the host and the client must download a VPN (Virtual Private Network) software. For this I used the free version of LogMeIn Hamachi This software allows other users to join my LAN from a remote location effectively putting us on the same network. After Hamachi is configured and I give the user my public IP address. We both can join the server by connecting to localhost or 127.0. 0.1. This method is nice because it requires little router configuration, but it does come at a cost. When I last used Hamachi, there was a free version that allowed 5 concurrent users on connect to you network. Now for my purposes at the time it was fine as only a handful of friends had wanted to play together. But should I require more VPN connections for more friends it would have costed me. However, for my purposes at this time is sufficed and we were able to play together despite no having access to my router settings.\nOption 3: Not that Easy but Convenient This option is what I use primarily from this point onwards as I have the more networking experience now and I also have access to my Router’s configurations. This option is a more traditional server style than the last. Since this server needs to actively communicate with machines located on different networks, I first need to ensure that the lines of transit are open. I will detail the required process for Windows computers as it is the method that I am most familiar with. First like the last step you need to download the server.jar executable from the Minecraft website. Next you need to open up the required port in you Router settings. If this port is left closed traffic from other computers will not be allowed to communicate with the server. To do this you must first land at your Router’s entry page. Similar to Option 1, you must run the ipconfig command and retrieve your default gateway. Typing this address into a web browser will allow you to change router settings. You must navigate to the networking option of the router that says either port-triggers or port-forwarding. It differs based on each make and model. Next you must allow the used port by adding a new entry to the port list. The port used by Minecraft is 25565, therefor the port range you should allow is 25565-25565 for both inbound and outbound to ensure only that port is opened. Next you must choose the protocol. It is recommended to allow both UDP and TCP protocol. With that done next you need to allow the port through the windows firewall as it is liable to block it on occasion. Enter the Windows Defender Firewall with Advanced Security tool through the start menu or control panel. Again, like the Router settings we need to allow UDP/TCP traffic both inbound and outbound. Once that is set up, you need to find your public IP address. I use https://www.whatsmyip.org/ for this as its easier than looking it up through Windows networking tools. Once that has been given to my friends, they are able to connect to the server when it is up. This option is by far my preferred method as I have the most control over everything involved. I can set up scripts to determine server uptime amongst many other useful things. However, it does require me to run the server in the background of my main machine or dedicate a separate machine to run the server, so its not perfect for all situations. My next and final option will go over “solving” this problem.\nOption 4: Very Easy, Very Convenient, but Costly Like many games, Minecraft allows you to pay to lease an official server. These are called Minecraft Realms. They offer all the benefits of the previous option without requiring the dedicated hardware to run it. However, it does cost a decent amount of money. To lease a server costs $7.99 monthly and while this seems cheap it does add up, given that the game itself is only $26.85 dollars. I have used this option for playing with others and it is convenient to be able to continue the fun even after the server owner decides to log off for the night, but for the cost I generally prefer to run my own server.\nConclusion In this blog I have gone over the methods I’ve used for running multiplayer sessions of Minecraft: Java Addition. It is worth mentioning that for other versions of this game all these steps may not be necessary. In the end I still prefer option 3 as it gives me the most control over the session and doesn’t cost anything. However, for someone not as experienced in networking, I can understand the value in some of the other methods. I hope this blog acts as a concise method of describing the networking options available for both Minecraft and perhaps other multiplayer online games, and provides options so people can choose the option that best caters to their needs.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog8/","tags":["CIT-480","Networking"],"title":"CIT 480 blog 8"},{"content":"Introduction This Blog will detail the trials and tribulations I faced while completing Lab 3. Lab 3 tasked us with yet again recompleting the steps of the prior lab autonomously. This lab required us to once again recreate the lamp stack using an Ansible playbook.\nAnsible Installing Ansible was very easy. I already had Python on my machine so I was able to easily install Ansible using the apt-get install ansible command. From that point I made a simple playbook that would run on my local host (127.0. 0.1) so that I could see if everything was functioning. I had it install PHP through apt. After I ran the Playbook I checked my PHP version and confirmed the playbook ran as intended.\nIssue 1: General Syntax Problems While reviewing my notes and documentation online I struggled briefly with the playbook syntax. The required spaces were a little tiresome, but the error messages did help me figure that out. Additionally, some commands took a little while to figure out the formatting of. At first, I wrote several Apt tasks before realizing it could all be done in a single task. Overall, this step took the longest but it was by no means difficult.\nIssue 2: PHP Installation As is a common theme with these labs I become too focused on one aspect that I make stupid mistakes. This lab I wanted to run my Playbook on some AWS EC2 instances. AWS is a tool that I really want to become acquainted with so I thought it was only logical to implement it into my projects when possible. Now here is the part where I experienced tunnel vision. I was reviewing the steps the Professor made when spinning up EC2 instances and followed them to a T. This included the image used. Despite being aware that the lab required 18.04 Ubuntu containers, I still focused on the latest available image on AWS. This would spell trouble with several PHP installations and certainly made feel foolish when the “aha” moment finally clicked. After selecting the proper instance and changing the IP’s in my hosts.ini file, the PHP installation continued without further issues.\nIssue 3: Affinity Installation First, I’d like to show off my first two attempts as well as my reasoning behind them Attempt 1: - name: Download Composer Installer get_url: url: https://getcomposer.org/installer dest: /installer mode: 777 become: true - name: Install Composer command: php -f installer --install-dir=/usr/local/bin --filename=composer become: true \nIn this attempt, my logic was to first download the installer using Ansible’s built in get_url module. Next, I would try to install it using the php -f command to install from a file. I chose to do this because in this case the command task would fail. My assumption was that the pipe broke it but I am unsure. In conclusion this did not work and to be honest I am not all too surprised. I’m not even certain the php command is formatted properly.\nAttempt 2: - name: Install Composer command: composer install args: chdir: /var/www/html/affinity become: true - name: Install Composer composer: command: install working_dir: /var/www/html/affinity become: true\nIn this attempt I invested much more time and I had higher hopes. While not native, apparently a user created Ansible plugin exists to install composer packages. I tried this is several different ways, using multiple different formats and versions. I even tried using deprecated versions to no avail. This led me to my final solution.\nAttempt 3 (The Winner): - name: Composer Install Script script: script.sh args: chdir: /var/www/html/affinity become: true\nscript.sh: #! /bin/bash  curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer composer install\nNow I do consider this somewhat “cheating” but in the long end it did work. I could not find a single way of accomplishing the composer installation via commands however I did discover a handy script module. From my research apparently this makes the process take a little longer, but it was the only solution I tried that worked. In the end I prefer a solution that isn’t 100% efficient over one that doesn’t work at all.\nIssue 4: Overthinking This mistake was barely an issue but so I’ll touch briefly on it. After Apache2 was installed it began running immediately. After all the configuration I bever restarted the service. This led to the CSUN tech labs page not displaying and causing a little panic. After I SSH’ed into my EC2 instance and restarted apache2 manually I discovered my mistake. I added a restart apache task to my playbook and everything worked as intended. To make sure everything was working altogether I spun up 2 entirely new EC2 Ubuntu 18.04 instances and tried the playbook fresh. I was greeted by 2 functioning splash pages so I knew my work was done.\nConclusion In the end I view this lab as a two-way learning experience. One I got to learn how to automate tasks on a remote machine using Ansible and two I was able to get some good AWS experience. Both of these skills I feel will be useful moving forward and I am starting to get ideas of how to incorporate these new technologies into my home life and scholastic projects. I may have struggled a bit more with this lab than prior ones, however it’s the struggle that makes finding the solution that much more enjoyable.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog7/","tags":["CIT-480","Ansible","PHP","Affinity"],"title":"CIT 480 blog 7"},{"content":"Introduction For this blog I actually have something more technical to write about. Since I am a CIT student and this is a fact my good friends are aware of, it is not unlikely for me to be summoned for computer help. In a saga of advice, I have given to my self-admitting technology illiterate friend I aided him in a peer to peer networking issue. I am always happy to help my friends and to stretch my “IT muscles.”\nA Little Background I have previously given this friend a lot of advice in the past. Some of this included the ways of running Windows applications on his MacBook. While my friend is very comfortable with Apple software, Windows is completely foreign to him. Through my guidance he installed bootcamp and had a working installation of Windows 10 on a separate hard drive partition. While troubleshooting issues he required elevated privileges so I instructed him on how to activate the administrator account with the net user administrator /active:yes. command. After a DirectX installation from Microsoft’s website, everything was functioning like a standard Windows 10 PC.\nThe Problem My friend wanted to play a multiplayer fighting game with a mutual friend, however they were experiencing networking issues because of the lack of dedicated servers. In order to play with friends, the game required a peer to peer connection. From their description, they determined that they needed to open a port on their router to allow the transmission. This seemed like a possibility so I went along with it.\nThe Method After realizing my instructions weren’t being followed, I decided to take matters into my own hands. I instructed the mutual friend to install (TeamViewer)[https://www.teamviewer.com/en-us/] I made sure to instruct him how anyone he gave his connection ID and password to would have full control over his PC. I am aware this ideally isn’t the remote-control application to use, however I thought it would be most accessible. First, I entered the control prompt to retrieve the router’s default gateway with the ipconfig command. I entered the gateway into the browser and accessed the Netgear landing page. I had prior experience with Netgear routers so I felt perfectly comfortable. I assumed that the user and password would be the default admin admin setup and I was correct. I instructed him the importance of changing these but I left that step up to him. However, when we progressed into the port triggering prompts, we ran into a wall. 3 security questions were shown, each with an answer set by his parents 4 years ago which they could not remember. At this point I told him his options were to either reset the router or we could try to continue without setting the router port triggers. He chose the second option.\nThe Next Attempt I thought there might be a chance that the port was already allowed through the router settings and maybe was getting blocked by the Windows Firewall. While I was investigating, I found 3 antivirus software downloaded concurrently, which I instructed the removal of the unused ones. I went into the Windows advanced firewall settings and created 4 new rules. 2 inbound rules that allowed traffic from the specific port one for UDP on for TCP, and 2 outbound rules for the same protocols. I chose to do both as I was unsure which protocol the game would use.\nThe Test After the rules were set, I instructed them to try the connection while I was still watching his screen. They navigated to the versus online screen and it prompted them for the network details. My first friend would be the host so he was the one tasked with providing his IP address. My friend began reading out his IP, “192.168-” I had to stop him there. This entire time I was trying to solve their problem when it was not even an issue. I told my friend that the IP address he was using was in fact his private IP. He was surprised to hear this because he said he tried the game with his brother earlier in the game and had no issues. His brother that was on a different computer about 5 ft away from him on the same network. I told that that connection only worked because they were on the same network and that is why the private IP address sufficed. After determining his IP address by directing him to https://www.whatismyip.com/what-is-my-public-ip-address/ he typed it in and it worked no problem.\nConclusion Overall, I find this a very good example of why IT is an important field. While my friend isn’t absolutely clueless like he claims there a few pitfalls that perhaps someone less knowledgeable in networking would fall into. It asked for his IP address so he naturally provided the first one he saw. I can’t blame him for thinking this as I had the same issue when I was younger and less experienced. This taught me take problems at surface value and to try to determine the solution myself rather than take other’s word for it. While these issues generally aren’t the hardest to solve, I always enjoy the opportunity to problem solve and help my friends along the way.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog6/","tags":["CIT-480","Networking","Troubleshooting"],"title":"CIT 480 blog 6"},{"content":"Introduction Although I thought I had little to discuss last blog regarding CIT, this week has been even less productive. While I have made progress in my computer science class, it was involving yet another HTML/CSS project. I have chosen to not talk about the same topic two postings in a row. Perhaps I will revisit the content of that class when we move on to browser scripts. This week beyond that I have been attending to my other non-major classes, so I have little to talk about that is both scholastic and technical. As a result, I will go over a personal project I recently did that does fit the technical aspect but not the scholastic one.\nThe Task There is a video game named Fight Night Round 2 released in 2005 on the Nintendo GameCube, PlayStation 2 and Xbox. The GameCube version of the game included a guest character named Little Mac from the Nintendo Punch-Out!! series. I am really into 3d printing and I wanted to get my hands on the 3d model of this character. The problem is that no common 3d model extractor in the community had bothered with this old 2005 clunky boxing game. This blog will detail the process I went through in order to acquire and optimize this model.\nGetting the Files The first thing I required was the games files. I was able to acquire the .ISO file of the disc by placing it into my Nintendo Wii and running some Homebrew software called USBLoaderGX link This software is used for legally acquiring the ISO files from owned video game discs for personal use only. Once I used the software on my legal copy of the game, I transferred the extracted files to my USB drive for further processing on my PC.\nRequired Tools From this point forward I would need a few tools:\n A method of playing the game on my PC A method of extracting the 3d model textures A method of extracting the 3d model itself  Luckily for me the GameCube Emulation community is thriving and I was able to use the Dolphin Emulator, (open source Wii/GameCube emulator) link to both play and extract the textures. The harder part would be to extract the model. I did some research and a tool called 3d Ripper Dx link This tool uses the DirectX 3d render engine to extract a single frame of an entire 3d scene.\nExtracting the Scene The first thing I needed to do was ensure that 3d Ripper Dx would work with the other software I was using. I researched the compatibly and 3d Ripper Dx only works with a deprecated version of Microsoft’s DirectX software as well as 32-bit programs. Luckily for me, the Dolphin emulator team provides experimental older builds that both support 32-bit operating systems and that old render engine. Once I configured the capture scene hotkey in 3d Ripper Dx I then opened the Dolphin Emulator and set the backend render engine to Direct3D9 (deprecated) While I knew this would affect performance, all that mattered was capturing the one scene. I tested the capture button and it functioned but was off. I’ll elaborate on that later.\nA Quick Aside Now in order to get my Desired Character in a Scene I first needed to unlock him. In order to unlock the character, you need to beat the included copy of the Super Nintendo Entertainment System game, Super Punch-out!! While I have done this before, playing on a poorly optimized render engine at subpar framerate was difficult. I still managed to do it after about an hour. Sorry for the little tangent, while it isn’t exactly technical, it was part of this process.\nNow that my character was unlocked, I played a match with him and at an opportune time I took the snapshot. There was a method to my timing. In order to make the character as maneuverable in a 3d space as possible, I wanted him to be in a certain pose. In the 3d animation industry this is known as a T-pose.\nThe closer he was to this position the easier processing would be later. Previously I mentioned that the snapshot was “off.” What I mean by that is that it was stretched and skewed bizarrely.\nThis is a direct result of method used to capture the model. 3d Ripper Dx uses the rendering engine to capture all polygons displayed within the screen’s capture. As a result, the angle of the camera will affect this outcome. To correct this, I brought the scene into my 3d modelling software of choice, Blender. Blender is a free, community driven tool for 3d model editing and creation. While I am still very much a novice at it, I was able to change the scale of the axis in order to get a somewhat acceptable result:\nConclusion Overall, I am somewhat happy with the result. While I do feel that my task was successful, I didn’t realize how rough the model would look by itself. Little Mac was not necessarily all that nice looking in the game, let alone with no textures seen in the image above. As a result, I have hesitated on preparing the character for 3d printing because I have realized that I really don’t want a physical figure of this oddly proportioned man. I still had fun during the process and learned something along the way, so I do not consider this project an absolute failure. I still have the model should I ever decide to move forward with the post-processing in the future.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog5/","tags":["CIT-480","3d-Models","Graphics-Engines"],"title":"CIT 480 blog 5"},{"content":"Introduction Since I have already completed the assignments thus far for CIT 480, I find myself at a lack of content to construct this blog about. As a result, I feel it’s appropriate to move into a more COMP 484 related direction. In this blog I will go over my first HTML/CSS Project for COMP 484 and the requirements/how I accomplished them.\nRequirements The project was simple, we had to construct an HTML website with 4 pages and a stylesheet. Since we were allowed to pick any topic we wanted. I chose the video game franchise, The Legend of Zelda as I was certain I could fill 4 pages with content. Additionally, we had to use 3 unique elements and 3 unique attributes we had yet to go over in class. The elements I chose were:\n Address Table Audio  I chose the Table element because I was already somewhat experienced with it and I felt that could allow me to provide better examples. Audio and Address were chosen because upon reading their documentation they seemed simple enough to implement while explaining my process.\nThe Attributes that I chose to cover extensively were:\n Align Autoplay Preload  I chose align because in previous work I found the text-align attribute to be somewhat limiting. Later I discovered that Align was deprecated, but I still used it because I had already committed and it still worked in HTML 5. Autoplay and Preload were chosen as they had functionality with the Audio element I had picked already. I would later run into some trouble with the Autoplay attribute.\nAddress The address field was simple enough, I just embedded the address between the address tags in my footer. I could see this being useful for accessibility. Potentially screen readers could use this tag to better represent webpage address information to people with sight disabilities. An example of the tag is below: \u0026lt;footer role=\u0026#34;contentinfo\u0026#34;\u0026gt; \u0026lt;br\u0026gt; \u0026lt;a href=\u0026#34;https://github.com/RobertFleming448/COMP484-Project1\u0026#34; title=\u0026#34;Introduction\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;assets/github.png\u0026#34; alt=\u0026#34;View On GitHub\u0026#34; width=\u0026#34;19%\u0026#34; height=\u0026#34;19%\u0026#34; \u0026gt;\u0026lt;/a\u0026gt; \u0026lt;br\u0026gt; \u0026lt;p\u0026gt; The Legend of Zelda \u0026amp;trade; Nintendo 1986-2020\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Nintendo of America Mailing Address:\u0026lt;/p\u0026gt; \u0026lt;address\u0026gt;4600 150th Ave NE Redmond, WA 98052 \u0026lt;/address\u0026gt; \u0026lt;a href=\u0026#34;#top\u0026#34; title=\u0026#34;Back to the top\u0026#34;\u0026gt;[TOP]\u0026lt;/a\u0026gt; \u0026lt;/footer\u0026gt;\nTable I used the Table tag in 2 ways.\n To structure Data in a certain way To structure the page in a certain way  My first approach structured data into a table consisting of the release date and information regarding each entry in the series. Here’s an example of the structure I used: ### The History of Zelda \u0026lt;table class=\u0026#34;timeline\u0026#34; id=\u0026#34;timeline\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th class=\u0026#34;timeline\u0026#34;\u0026gt;Title:\u0026lt;/th\u0026gt; \u0026lt;th class=\u0026#34;timeline\u0026#34;\u0026gt;Description\u0026lt;/th\u0026gt; \u0026lt;th class=\u0026#34;timeline\u0026#34;\u0026gt;Original Release Date:\u0026lt;/th\u0026gt; \u0026lt;th class=\u0026#34;timeline\u0026#34;\u0026gt;Platforms:\u0026lt;/th\u0026gt; \u0026lt;th class=\u0026#34;timeline\u0026#34;\u0026gt;Screenshot:\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\u0026#34;timeline\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;The Legend of Zelda\u0026lt;/h4\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td class=\u0026#34;timeline\u0026#34;\u0026gt;Collect the 8 fragments of the Triforce of Wisdom to defeat Ganon and rescue Princess Zelda\u0026lt;/td\u0026gt; \u0026lt;td class=\u0026#34;timeline\u0026#34;\u0026gt;1986\u0026lt;/td\u0026gt; \u0026lt;td class=\u0026#34;timeline\u0026#34;\u0026gt;Nintendo\u0026amp;nbsp;Entertainment\u0026amp;nbsp;System,\u0026lt;br\u0026gt;Gameboy Advance,\u0026lt;br\u0026gt;GameCube,\u0026lt;br\u0026gt;Wii\u0026lt;br\u0026gt;3ds,\u0026lt;br\u0026gt;Wii U,\u0026lt;br\u0026gt;Nintendo Switch\u0026lt;/td\u0026gt; \u0026lt;td class=\u0026#34;timeline\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;assets/timeline/zelda1.png\u0026#34; alt=\u0026#34;Zelda 1 Picture\u0026#34; width=\u0026#34;400\u0026#34; height=\u0026#34;300\u0026#34;\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt;\nWith the style sheet enabled it looked like this:\nMy second use was to structure the page to contain a description on one side and an image on the other. Here is an example of that structure: \u0026lt;table class=\u0026#34;center\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\u0026#34;desc\u0026#34; id=\u0026#34;link\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;Link\u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt;A courageous young lad clad in green. He wields the \u0026lt;a href=\u0026#34;lore.html#courage\u0026#34; title=\u0026#34;Triforce of Courage\u0026#34;\u0026gt;Triforce of Courage\u0026lt;/a\u0026gt; and the Master Sword. Using his wits and everlasting courage he endeavors to bring peace back to the land of Hyrule. While there are many heroes that go by the name Link, they all share one thing in common, the spirit of the hero.\u0026lt;/p\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td class=\u0026#34;pic\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;assets/characters/link.png\u0026#34; alt=\u0026#34;Link Picture\u0026#34; width=\u0026#34;400\u0026#34; height=\u0026#34;400\u0026#34;\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\u0026#34;desc\u0026#34; id=\u0026#34;zelda\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;Zelda\u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt;The Princess of Hyrule\u0026#39;s royal family. She possesses the \u0026lt;a href=\u0026#34;lore.html#wisdom\u0026#34; title=\u0026#34;Triforce of Wisdom\u0026#34;\u0026gt;Triforce of Wisdom\u0026lt;/a\u0026gt; which enhances her existent supernatural abilities. Legends say that the Princess is a descendant of the Goddess herself. Zelda assists Link in defeating evil in whichever era it may reside.\u0026lt;/p\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;assets/characters/zelda.png\u0026#34; alt=\u0026#34;Zelda Picture\u0026#34; width=\u0026#34;400\u0026#34; height=\u0026#34;400\u0026#34;\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt;\nAnd the associated image output:\nAudio Of all the elements this was the easiest to implement. Similar to other media tags all I had to do was specify the source and the type like so: \u0026lt;audio controls autoplay loop\u0026gt; \u0026lt;source src=\u0026#34;audio/sacred_realm.mp3\u0026#34; type=\u0026#34;audio/mpeg\u0026#34; preload=\u0026#34;auto\u0026#34; \u0026gt; Audio Not supported by your browser \u0026lt;/audio\u0026gt;\nThe result is the following:\nThe Attributes Align was used on several pages in order to arrange information/media that was not text based. I used it on my audio, video, and images in order to justify them on the page where I wanted them.\nAutoplay was used twice due to a lack of thinking initially. I first used it on my audio tag but I had failed to realize that modern browsers block media autoplay if it’s not muted (rightfully so) To demonstrate my knowledge of the tag I instead added a muted mp4 to a separate page, which displayed as intended.\nPreload was used to ensure certain media would be loaded automatically as the page loaded. My example was an mp4 file that I wanted to play as a sort of welcome banner. I wanted it to capture the attention of the user upon first visit so I made it load automatically.\nThe Style Sheet The professor did not necessarily specify his preferred format for the CSS, be it in line, in document, or separate. As a precaution I just chose to make it separate. This helped me maintain stylistic unity on all my pages. To make the CSS I first started with the content of one of the professor’s sample pages. From there I tweaked the stylesheet until the desired look was attained. After that I structured my pages with filler words (ipsum lorem\u0026hellip;) to get the general idea of the page flow. I would later make small tweaks to my CSS to account for special page formatting but it maintained pretty consistent after the initial draft.\nConclusion In the end I created a website that I was proud of. It doesn’t have any script functionality, but I feel I was able to construct a nice-looking page that abided by the requirements set. If you want to see my website, you can see it here on github. Overall, I had fun making this site and it was cool getting to use graphical elements as most of my projects are either very ugly or non-graphical at all.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog4/","tags":["CIT-480","HTML","GitHub"],"title":"CIT 480 blog 4"},{"content":"Introduction This blog will detail my struggles and procedures used while attempting the second lab for CIT 480. Much of this lab was very similar to the procedures of Lab 1, since the goal was to make a docker file to replicate the Lab 1 environment on demand. Given that this lab would contain similar content to my previous write-up, I will instead go into detail the hiccups I encountered while trying to complete the lab and how I eventually overcame them.\nProblem Number 1. Since this lab was heavily based on Lab 1, I was refollowing Lab 1’s instructions to a T. However, one notable difference was required that I had not considered. During Lab 1 we created a docker container with the docker run -it -p 8080:80 –name lab1 ubuntu:18.04 bash That command created a docker container using the ubuntu 18.04 image, with internal port 80 mapped to external port 8080 named lab1. Even though I understood the command in full, there was one main problem I encountered and it had to do with the port-mapping. While I was creating the docker file I was using the previous lab manual as well as my finished container as reference. Since the goal was to recreate the previous environment 1:1 I thought it would be logical to use the same port numbers. Again, in hindsight this makes little sense. After searching online for a solution to the bind for 0.0.0.0:8080 failed: port is already allocated I encountered a forum post stating that since the port was already allocated to an active container, it could not be used again. It seemed so obvious and I knew I had 2 options:\n stop the active container so the new container can be allocated the port give the new container another port  I chose option 2 so that I may cross reference both at the same time. With that problem solved, I tested an apache install to ensure the service was using the correct port, which it did. Hurray\nProblem Number 2. This problem was both easy to solve and simple. Like last lab I encountered difficulty when installing multiple PHP modules at once. I still think this was some strange dependency issue but I am unsure. To get around this I just used several apt-get install commands rather than one long one. While this did increase the length of my Docker file, PHP was able to install autonomously with no issue.\nProblem Number 3. This problem while less obvious to me had yet another simple fix. While I had no issue using the RUN command in the docker file due to the DEBIAN_FRONTEND env variable being set to noninteractive, I encountered some difficulty when trying to run a command that required a certain directory. I had assumed that the RUN cd \u0026lt;dir name\u0026gt; command would provide me with the desired results to change the working directory, yet when I the prompt informed me that certain directories could not be found in future steps, I was confused. I started by using the full directory but this was no longer possible when it came to the Affinity installation step. Looking on forum posts I learned that the RUN cd \u0026lt;dir name\u0026gt; command did not worked how I thought it did from within a docker file. Instead to change the working directory, you had to use the WORKDIR \u0026lt;dire name\u0026gt; instead. After I swapped every RUN cd \u0026lt;dir name\u0026gt; for its corresponding WORKDIR \u0026lt;dir name\u0026gt; everything worked like a charm and I was greeted with the glorious CSUN Web Service Page after I started the apache2 service.\nConclusion Overall, I found this lab to be a real learning experience. I have done tasks similar to this one that involved automating tasks via scripts and those were mostly straight-forward. Even with docker I have set up my container to a predetermined configuration by copy and pasting a script into a file. However, this was my first time using a docker file. From my experience this is much better than the previous methods I had used to construct my container environment. I especially see the use of building a docker image that will see many repeated docker containers uses. My previous script method would take far longer should I need to replicate my environment periodically. Lastly, I learned that most mistakes can be caused by just being careless. This is especially evident in my Problem Number 1. where even though I had a complete understanding of the requirements, I still made a mistake that cost me time. As of now, my docker file could be submitted but I am still editing it to ensure that there is nothing superfluous included. Given that I still have time to work on it before the deadline, I will continue optimizing it during time I’ve allotted to CIT 480 lab work.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog3/","tags":["CIT-480","Docker"],"title":"CIT 480 blog 3"},{"content":"Introduction This blog will document the processes and tribulations I faced while completing Lab 1 Lamp Stack. I did not anticipate much trouble with this lab since I have done a very similar process hosting an apache webserver on an ubuntu container in a previous class. However, the task was slightly different and as a result had specific requirements.\nDocker Docker was a tool I have used in the past scholastically. While my previous docker installation/environment was no longer available I did not anticipate much trouble setting it up again. I ran into a slight hiccup but that was to be expected.\nDocker Installation Again, I began this lab with the intention of completing it in my familiar windows 10 pro environment as I am most comfortable on this OS. However, I ran into certain difficulties. Since I am running the Pro version of Windows 10 I anticipated the standard docker installation that utilizes the Pro exclusive Hyper-V feature. I have had some issues in the past with this feature using Oracle VM Virtual Box, but since I had those problems fixed, I thought the docker setup would be simple. Immediately after installing docker from their website, it failed to activate the windows Hyper-V feature. I had already confirmed that my machine enabled virtualization via the BIOS and I did not want to troubleshoot the issue so I immediately moved to my Ubuntu environment. I am aware that Docker toolbox can function with Windows 10 Home features but I did not want to tinker with that when I knew it would work instantly on my Ubuntu Environment. After adding the Docker repository with:\n$ sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34; I was able to install it with a few simple commands: sudo apt-get update and sudo apt-get install docker-ce docker-ce-cli containerd.io I will endeavor in the future to start labs on my ubuntu environment to save myself the headache.\nDocker Configuration While I was already familiar with docker, I wanted to get a feel for it again so I perused the available commands and briefly reviewed them. After that I tried out the Docker hello world image to see if everything was functioning as expected. Luckily it was so I pulled the most recent image of Ubuntu and created a container with the docker run -it -p 8080:80 --name lab1 ubuntu:18.04 bash command specified in the lab. As explained by the professor during lecture, the 8080:80 section is used to map local ports to ports from within the container. After that I set up a Docker user so I no longer had to run every command with root privileges.\nCreating the LAMP Stack Prepping the Container I refreshed the list of apt repositories via the apt-get update \u0026amp;\u0026amp; apt-get upgrade command. Next, I installed git curl zip and vim using the apt-package manager. I chose vim over nano because personally I am just more familiar with it.\nOther LAMP Installations Next, also using apt I installed apache and navigated to its configuration directory. After installation I ran the apache service and visited the localhost page in my browser and was greeted with the apache2 default page. Next, I installed the required MySQL modules using the prespecified commands. After confirming each module installed correctly, I moved on with the PHP installation. The one-liner provided in the lab for some reason was not working as expected so I manually installed each module separately. After these were all finished installing, I was able to confirm all required modules were operational.\nLAMP Configuration Since the webpage we were making is being constructed via PHP we needed to change the order in the config file to give it precedence. Next, I created the info.php file: \u0026lt;?php phpinfo(); ?\u0026gt; Upon restarting the apache2 service and navigating to the info.php page on my localhost I was able to view the php info page.\nCloning Application Repository After removing the php.info page I navigated to the /var/www/html directory. Next, I cloned the CSUN tech lab affinity repository. Next, I changed ownership of the affinity directory and created a sym link. Next, I went into the configuration files and made the prespecified changes; I restarted apache so the changes could take effect. I downloaded composer via curl navigated to the affinity directory and installed composer. Upon refreshing my web browser displaying localhost I was greeted with desired CSUN Affinity web service page. Lastly, I ran the curl command and took screenshots of both as my submission.\nConclusion This lab was somewhat of a refresher for me. In the past I have already used Docker for school projects. Additionally, I had a brief introduction to PHP so I look forward to expanding on that should that be the path we continue on. The most difficulty I encountered was Docker installation failing but that was rather simple to avoid. I also struggled momentarily with the php module one-liner but was able to install them without much hassle. Overall, I am glad we did this lab as it gave me time to get reacquainted with Docker and LAMP functionality.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog2/","tags":["CIT-480","Docker","PHP"],"title":"CIT 480 Blog 2"},{"content":"Introduction Given as this is my first officially numbered blog, I will now make the contents of this blog pertaining to my endeavors of class material. As a result, for this blog I will be primarily focusing on my experience setting up Git on my machine as well as Jekyll with its supporting modules.\nGit The first step was configuring Git on my home computer. I have used Git before for work-related source code management, but I have never used it on my personal computer. So, while I was familiar with the standard git bash commands git add, git commit, git push etc. I had never configured a repository nor set up a clone before.\nCloning the Repository I first decided I was going to use my windows 10 OS for configuring settings and completing assignments however I changed my mind later down the road for reasons I will specify in a future section. I had initially downloaded the Github GUI application and after cloning the repository via https it worked similarly to the command line variant, I was already familiar with. The installation and cloning on my Ubuntu environment were also simple via the apt package manager tool.\nGit Configuration After several test commits of a separate repository, I grew wary of inputting my credentials on my Ubuntu environment. I ran the git config --global credential.helper store command before my next commit to save time for future commits. Beyond that, very little extra configurations were done to my Git environment.\nJekyll Installing and configuring Jekyll was the next step in order to make quick easy blog posts for this class via markup language. However, Jekyll’s installation process was more in depth than Git’s and had a few more steps.\nInstallation I had initially tried installing Jekyll on my Windows 10 environment despite knowing there would be several more steps. First, I had to install Ruby which I sourced easily from rubyinstaller.org\nI had initially tried using the windows bash method but given that apt was unavailable I thought the Ruby method would be easier. After Ruby finished installing, I installed bundler which appeared to function as intended as identified by its complete status. Next, I installed Jekyll and it appeared to function as intended providing the correct version number after installation. When I continued to generate a new blog however the process continually failed. I wasn’t sure if it was a permissions problem with my folder or if the Ruby shell could not find the proper path but I decided perhaps this would be easier on my Ubuntu environment so I swapped over. The ubuntu installation was much easier and after a series of three or so commands Jekyll had generated the required files to make a blog.\nConfiguration Jekyll configuration was mostly to the point. I had to edit the _config.yml and Gemfile to get the page working from Gitpages. Later however a setting had to be reverted because it caused the Gitpage to load incorrectly. Lastly, I edited the index page to link to all future blogs so that I may access them easier in the future. The standard index page at some point just stopped loading and I am unaware of the cause so I decided to link every blog post manually and edit the dates/titles when appropriate.\nWriting the Blog Using the sample markup file, I was able to figure out the markup structure rather easily. A few google searches and looks through documentation helped me figure out how to format lists and code snippets to make the blog really pop.\nBuilding the Page I finally committed my changes and pushed the project but yet nothing appeared. Git had indicated that the build was a success, yet my page was no where to be found. After looking through Git’s repository settings I was able to find the source directory setting and set it to the \\_docs directory. Additionally, I changed my theme which further improved the look of the blog.\nConclusion Overall, this process was a good learning opportunity. I was able to set up a Git repository and clone it myself rather than work on a previously configured Git environment. I also was able to go through the setup of Jekyll, ruby, and bundler on two separate operating systems, with varying levels of success. I feel that the hardest part of this process was actually getting Jekyll to cooperate with Gitpages, and more documentation would have been appreciated. While at this point, I feel like publishing to Gitpages in pure HTML would have been simpler, I do appreciate how quickly Jekyll blogs can be written. The level of consistency blogposts can achieve with Jekyll and the rapid development does offset the steeper configuration learning curve.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog1/","tags":["CIT-480","git","Jekyll"],"title":"CIT 480 blog 1"},{"content":"Introduction Since this is my first blog post for CIT 480 and I have little to discuss in terms of class content, I think it would be fitting to describe the process I used to enroll in COMP 484 and this class. When I went to enroll in this course\u0026rsquo;s corequisite, COMP 484, it was closed. I\u0026rsquo;ve been through this before and the only solution I\u0026rsquo;ve found is to refresh the page every couple of minutes to wait for availability. This would be no problem since this occurred in midst of Covid stay at home orders. However, I would be unavailable to refresh this during meals, bathing, etc. To solve this, I came up with a system to automize the refresh system.\nI decided to write this program in Java, as it was the programming language I was most familiar with. First off, I devised a logical breakdown for the program. The required modules were:\n A bot that refreshed the page and navigated to the add screen A function that captured the page A function that parsed the image to see if any changes were made to the add screen A function that notifies me when a change occurs, so I can run to my computer  Next are a few \u0026ldquo;optional\u0026rdquo; components that I included to make the program easier to bug correct\n A logging function that appends whenever a significant action occurs including time and picture Bug correction function that rescues the bot should it get stuck during page navigation  The Bot I first made a bot that would navigate the page by pressing the TAB key a predetermined amount of times and pressing ENTER when required. To accomplish this, I used the robot class located in the java.awt library. This method encountered far too many infinite \u0026ldquo;death loops\u0026rdquo; when the bot got stuck on an incorrect page so I scrapped it.\nThe second bot while less portable, used the same Robot class as before. To ensure the bot never fell into an infinite loop, I hardcoded the bot to navigate using the mouse cursor using the measurements of my screen. This was not an elegant solution but for my purposes it functioned. The bot was capable of scrolling through the pages pressing the required radio bubble and waiting at the add screen before restarting the process. The code for this bot is below:\npublic static void refreshPageAlt() throws AWTException, InterruptedException { Robot robot = new Robot(); TimeUnit.SECONDS.sleep(2); robot.keyPress(KeyEvent.VK_F5); robot.keyRelease(KeyEvent.VK_F5); TimeUnit.SECONDS.sleep(8); //first Page \trobot.mouseMove(700, 200); TimeUnit.SECONDS.sleep(1); robot.mousePress(InputEvent.BUTTON1_DOWN_MASK); robot.mouseRelease(InputEvent.BUTTON1_DOWN_MASK); TimeUnit.SECONDS.sleep(1); robot.mouseMove(1910, 300); robot.mousePress(InputEvent.BUTTON1_DOWN_MASK); robot.mouseMove(1910, 1000); robot.mouseRelease(InputEvent.BUTTON1_DOWN_MASK); TimeUnit.SECONDS.sleep(1); robot.mouseMove(700, 940); TimeUnit.SECONDS.sleep(1); robot.mousePress(InputEvent.BUTTON1_DOWN_MASK); robot.mouseRelease(InputEvent.BUTTON1_DOWN_MASK); TimeUnit.SECONDS.sleep(8); //Second Page \trobot.mouseMove(220, 450);\t//alt screen size: (220, 450)\t//main screen size(220, 470) \trobot.mousePress(InputEvent.BUTTON1_DOWN_MASK); robot.mouseRelease(InputEvent.BUTTON1_DOWN_MASK); TimeUnit.SECONDS.sleep(1); robot.mouseMove(600, 480);\t//alt screen size: (600, 480)\t//main screen size(600, 500) \trobot.mousePress(InputEvent.BUTTON1_DOWN_MASK); robot.mouseRelease(InputEvent.BUTTON1_DOWN_MASK); append(\u0026#34;Page Refreshed\u0026#34;); System.out.println(\u0026#34;Page Refreshed\u0026#34;); TimeUnit.SECONDS.sleep(8); } The Capture/Analyzer Next, I had to come up with a function that would take the contents of the screen and determine if a class became available. Luckily CSUN\u0026rsquo;s system color codes classes with the following scheme:\n Green=Available Yellow=Waitlisted Blue=Closed  Using these colors, I could easily determine changes in class availability based on color information. Utilizing the Color class and the Rectangle class I was able to take a screenshot of the page and get the RGB values of each pixel. By measuring the changes in green pixels on screen from previous measurements I could determine if availability changed. Part of the code is below:\npublic static void capture() { try { Robot robot = new Robot(); //Fullscreen but cropped around class hardcoded for my screen  Rectangle screenSize = new Rectangle(Toolkit.getDefaultToolkit().getScreenSize()); //System.out.println(screenSize.width);  //System.out.println(screenSize.height);  Rectangle captureRect = new Rectangle(420, 1130 / 2, 581 , 448);\t//alt screen dimensions: (420, 1130 / 2, 581 , 448);\t//main screen size (420, 1150 / 2, 581 , 448 );  BufferedImage bufferedImage = robot.createScreenCapture(captureRect); File file = new File(\u0026#34;screen-capture.png\u0026#34;);\tboolean status = ImageIO.write(bufferedImage, \u0026#34;png\u0026#34;, file); append(\u0026#34;Screen Captured ? \u0026#34; + status + \u0026#34; File Path:- \u0026#34; + file.getAbsolutePath()); System.out.println(\u0026#34;Screen Captured ? \u0026#34; + status + \u0026#34; File Path:- \u0026#34; + file.getAbsolutePath()); } } public static int parseImg() throws IOException { int greenCount=0; //Reading the image \tBufferedImage img = ImageIO.read(new File(\u0026#34;screen-capture.png\u0026#34;)); for (int y = 0; y \u0026lt; img.getHeight(); y++) { for (int x = 0; x \u0026lt; img.getWidth(); x++) { //Retrieving contents of a pixel \tint pixel = img.getRGB(x,y); //Creating a Color object from pixel value \tColor color = new Color(pixel, true); //Retrieving the R G B values \tint blue = color.getBlue(); if (blue\u0026lt;50) { greenCount++; } } } return greenCount; } Notification Next, I needed to be able to know if a change had occurred. There were 2 methods I thought of that could accomplish this task:\n Using a SMTP server to utilize my cell provider\u0026rsquo;s email to SMS service address Using a third-party service and paying per SMS message  I had initially started with the email method but after some configuration and thought I determined I didn\u0026rsquo;t have the resources to run a SMTP server on my computer. In the end I used a payed service called Twilio Twilio provides the libraries required to use their SMS services in dozens of computer languages, Java included. In the end from testing and utilization the service cost me a little over a dollar, with each text costing a fraction of a cent.\nError Correction Every so often, the program would get stuck in a loop that I had not accounted for. This was not surprising as many parts of the bot were hardcoded for my environment. To fix this I added the link to the add page to the program. After encountering too many error values, the bot would rescue itself by returning to the starting page. While this solution was not the most elegant, again it worked for my purposes.\nLogging This was by far the easiest function to implement. The method would just append the programs current status and time to a text document. Additionally, it would save a screenshot titled the current time. If an error were to occur, I could read the log and look at the corresponding image to try and determine the cause.\nConclusion In the end the program was a rousing success. I had turned the program on while I was eating lunch and low and behold a notification. I ran to my computer and found the class space open. Without this notification there is no doubt this availability would have been snatched up before I noticed. For this reason, I think the time and money I invested in this program was well worth it.\n","date":"2021-02-17","permalink":"https://robertfleming448.github.io/post/480blog0/","tags":["CIT-480","Java"],"title":"CIT 480 Blog 0"}]